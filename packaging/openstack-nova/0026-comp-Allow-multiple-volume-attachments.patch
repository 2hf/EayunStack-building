From bca8e9d95408b8867c85091405a58505f0161354 Mon Sep 17 00:00:00 2001
From: Krisztian Gacsal <krisztian.gacsal@ericsson.com>
Date: Thu, 18 Jun 2015 15:08:20 +0200
Subject: [PATCH 26/34] comp: Allow multiple volume attachments

Handle multiple volume attachments in nova-compute and adopt cinder driver
multiattach support.

Partially-implements: blueprint multi-attach-volume

Related patches:

REST API changes:
https://review.openstack.org/153038

python-cinderclient:
https://review.openstack.org/#/c/85856/

cinder:
https://review.openstack.org/#/c/85847/

Change-Id: I3cdc49924acbdd21d1e6678a3bb4cf7de7f1db1a
(cherry picked from commit 41c900c942e508e4b8c657398f3f4b22ad88e1dd)

Conflicts:
	nova/compute/api.py
	nova/tests/unit/api/ec2/test_cinder_cloud.py
	nova/tests/unit/compute/test_compute.py
	nova/tests/unit/compute/test_compute_api.py
	nova/tests/unit/compute/test_compute_mgr.py
	nova/tests/unit/fake_volume.py
	nova/tests/unit/test_cinder.py
	nova/tests/unit/volume/test_cinder.py
	nova/volume/cinder.py
---
 nova/compute/api.py                     |  20 ++++-
 nova/compute/manager.py                 |  32 +++++---
 nova/compute/rpcapi.py                  |   7 +-
 nova/tests/api/ec2/test_cinder_cloud.py |  99 ++++++++++++++++-------
 nova/tests/compute/test_compute.py      | 134 ++++++++++++++++++++++----------
 nova/tests/compute/test_compute_api.py  |   6 +-
 nova/tests/compute/test_compute_mgr.py  |  30 ++++---
 nova/tests/fake_volume.py               |  24 ++++--
 nova/tests/volume/test_cinder.py        |  33 ++++++--
 nova/volume/cinder.py                   |  98 +++++++++++++++--------
 10 files changed, 343 insertions(+), 140 deletions(-)

diff --git a/nova/compute/api.py b/nova/compute/api.py
index 4b71257..ad27165 100644
--- a/nova/compute/api.py
+++ b/nova/compute/api.py
@@ -1727,7 +1727,11 @@ class API(base.Base):
                     self.volume_api.terminate_connection(context,
                                                          bdm.volume_id,
                                                          connector)
-                    self.volume_api.detach(elevated, bdm.volume_id)
+                    volume = self.volume_api.get(bdm.volume_id)
+                    attachment = self.volume_api.get_volume_attachment(
+                        volume, instance.uuid)
+                    self.volume_api.detach(elevated, bdm.volume_id,
+                                           attachment['id'])
                     if bdm.delete_on_termination:
                         self.volume_api.delete(context, bdm.volume_id)
                 except Exception as exc:
@@ -2943,10 +2947,18 @@ class API(base.Base):
         if volume['attach_status'] == 'detached':
             msg = _("Volume must be attached in order to detach.")
             raise exception.InvalidVolume(reason=msg)
-        # The caller likely got the instance from volume['instance_uuid']
-        # in the first place, but let's sanity check.
-        if volume['instance_uuid'] != instance['uuid']:
+
+        # Volumes may be attached to multiple hosts and/or instances,
+        # so just check the attachment specific to this instance
+        if volume['attachments']:
+            for attachment in volume['attachments']:
+                if attachment['instance_uuid'] == instance.uuid:
+                    break
+            else:
+                raise exception.VolumeUnattached(volume_id=volume['id'])
+        else:
             raise exception.VolumeUnattached(volume_id=volume['id'])
+
         self._detach_volume(context, instance, volume)
 
     @wrap_check_policy
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index e3deb57..2c19311 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -2385,7 +2385,12 @@ class ComputeManager(manager.Manager):
                 self.volume_api.terminate_connection(context,
                                                      bdm.volume_id,
                                                      connector)
-                self.volume_api.detach(context, bdm.volume_id)
+                volume = self.volume_api.get(context, bdm.volume_id)
+                # find the attachment id for the instance.
+                attachment = self.volume_api.get_volume_attachment(
+                    volume, instance['uuid'])
+                self.volume_api.detach(context, bdm.volume_id,
+                                       attachment['attachment_id'])
             except exception.DiskNotFound as exc:
                 LOG.debug('Ignoring DiskNotFound: %s', exc,
                           instance=instance)
@@ -4526,8 +4531,9 @@ class ComputeManager(manager.Manager):
                       instance, bdm=None):
         """Attach a volume to an instance."""
         if not bdm:
-            bdm = objects.BlockDeviceMapping.get_by_volume_id(
-                    context, volume_id)
+            bdm = objects.BlockDeviceMapping.\
+                    get_by_instance_and_volume_id(context, volume_id,
+                                                  instance.uuid)
         driver_bdm = driver_block_device.DriverVolumeBlockDevice(bdm)
 
         @utils.synchronized(instance.uuid)
@@ -4602,8 +4608,9 @@ class ComputeManager(manager.Manager):
     @wrap_instance_fault
     def detach_volume(self, context, volume_id, instance):
         """Detach a volume from an instance."""
-        bdm = objects.BlockDeviceMapping.get_by_volume_id(
-                context, volume_id)
+        bdm = objects.BlockDeviceMapping.\
+                get_by_instance_and_volume_id(context, volume_id,
+                                              instance.uuid)
         if CONF.volume_usage_poll_interval > 0:
             vol_stats = []
             mp = bdm.device_name
@@ -4627,12 +4634,16 @@ class ComputeManager(manager.Manager):
 
         self._detach_volume(context, instance, bdm)
         connector = self.driver.get_volume_connector(instance)
+        volume = self.volume_api.get(context, volume_id)
         self.volume_api.terminate_connection(context, volume_id, connector)
+        attachment = self.volume_api.get_volume_attachment(volume,
+                                                           instance.uuid)
         bdm.destroy()
         info = dict(volume_id=volume_id)
         self._notify_about_instance_usage(
             context, instance, "volume.detach", extra_usage_info=info)
-        self.volume_api.detach(context.elevated(), volume_id)
+        self.volume_api.detach(context.elevated(), volume_id,
+            attachment['attachment_id'])
 
     def _init_volume_connection(self, context, new_volume_id,
                                 old_volume_id, connector, instance, bdm):
@@ -4707,8 +4718,9 @@ class ComputeManager(manager.Manager):
         """Swap volume for an instance."""
         context = context.elevated()
 
-        bdm = objects.BlockDeviceMapping.get_by_volume_id(
-                context, old_volume_id, instance_uuid=instance.uuid)
+        bdm = objects.BlockDeviceMapping.get_by_instance_and_volume_id(
+                                                context, old_volume_id,
+                                                instance.uuid)
         connector = self.driver.get_volume_connector(instance)
         comp_ret, new_cinfo = self._swap_volume(context, instance,
                                                          bdm,
@@ -4747,8 +4759,8 @@ class ComputeManager(manager.Manager):
                     expected_attrs=metas)
             instance._context = context
         try:
-            bdm = objects.BlockDeviceMapping.get_by_volume_id(
-                    context, volume_id)
+            bdm = objects.BlockDeviceMapping.get_by_instance_and_volume_id(
+                    context, volume_id, instance.uuid)
             self._detach_volume(context, instance, bdm)
             connector = self.driver.get_volume_connector(instance)
             self.volume_api.terminate_connection(context, volume_id, connector)
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index 34466ac..cc2d8e1 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -751,9 +751,10 @@ class ComputeAPI(object):
         cctxt = self.client.prepare(server=_compute_host(None, instance),
                 version=version)
         volume_bdm = cctxt.call(ctxt, 'reserve_block_device_name', **kw)
-        if not isinstance(volume_bdm, objects.BlockDeviceMapping):
-            volume_bdm = objects.BlockDeviceMapping.get_by_volume_id(
-                ctxt, volume_id)
+        bdm_cls = objects.BlockDeviceMapping
+        if not isinstance(volume_bdm, bdm_cls):
+            volume_bdm = bdm_cls.get_by_instance_and_volume_id(
+                ctxt, volume_id, instance.uuid)
         return volume_bdm
 
     def backup_instance(self, ctxt, instance, image_id, backup_type,
diff --git a/nova/tests/api/ec2/test_cinder_cloud.py b/nova/tests/api/ec2/test_cinder_cloud.py
index 8c48777..73a38cc 100644
--- a/nova/tests/api/ec2/test_cinder_cloud.py
+++ b/nova/tests/api/ec2/test_cinder_cloud.py
@@ -301,17 +301,40 @@ class CinderCloudTestCase(test.TestCase):
         vol1_uuid = ec2utils.ec2_vol_id_to_uuid(vol1['volumeId'])
         kwargs = {'image_id': 'ami-1',
                   'instance_type': CONF.default_flavor,
-                  'max_count': 1,
-                  'block_device_mapping': [{'device_name': '/dev/sdb',
-                                            'volume_id': vol1_uuid,
-                                            'delete_on_termination': True}]}
-        self._run_instance(**kwargs)
-        resp = self.cloud.detach_volume(self.context,
-                                        vol1['volumeId'])
+                  'max_count': 1}
+        ec2_instance_id = self._run_instance(**kwargs)
+
+        # NOTE(ft): Since fake detach action is very fast, we replace it to
+        # empty function to check EC2 API results at 'detaching' stage
 
-        # Here,the status should be 'detaching',but it can be 'detached' in
-        # unittest scenario if the detach action is very fast.
-        self.assertIn(resp['status'], ('detaching', 'detached'))
+        vol1 = self.cloud.attach_volume(self.context,
+                                        vol1['volumeId'],
+                                        ec2_instance_id,
+                                        '/dev/sde')
+
+        def fake_detach_volume(context, ec2_vol_id, instance_id):
+            vol_uuid = ec2utils.ec2_vol_id_to_uuid(ec2_vol_id)
+            volume = self.volume_api.get(context, vol_uuid)
+            volume['attachments'][0]['state'] = 'detaching'
+            ec2_vol = self.cloud._format_volume(context, volume)
+            return ec2_vol
+
+        self.stubs.Set(self.cloud, 'detach_volume', fake_detach_volume)
+
+        resp = self.cloud.detach_volume(self.context,
+                                        vol1['volumeId'],
+                                        ec2_instance_id)
+
+        self.assertEqual('in-use', resp['status'])
+        resp = self.cloud.describe_volumes(self.context, [vol1['volumeId']])
+        volume = resp['volumeSet'][0]
+        self.assertEqual('in-use', volume['status'])
+        # device and instanceId
+        self.assertThat({'status': 'attached',
+                         'volumeId': 'vol-00000001',
+                         'device': None,
+                         'instanceId': None},
+                         matchers.IsSubDictOf(volume['attachmentSet'][0]))
 
     def test_describe_snapshots(self):
         # Makes sure describe_snapshots works and filters results.
@@ -801,8 +824,9 @@ class CinderCloudTestCase(test.TestCase):
         return self.volume_api.create_with_kwargs(self.context, **kwargs)
 
     def _assert_volume_attached(self, vol, instance_uuid, mountpoint):
-        self.assertEqual(vol['instance_uuid'], instance_uuid)
-        self.assertEqual(vol['mountpoint'], mountpoint)
+        self.assertEqual(vol['attachments'][0].get('instance_uuid'),
+                         instance_uuid)
+        self.assertEqual(vol['attachments'][0].get('mountpoint'), mountpoint)
         self.assertEqual(vol['status'], "in-use")
         self.assertEqual(vol['attach_status'], "attached")
 
@@ -839,8 +863,12 @@ class CinderCloudTestCase(test.TestCase):
         ec2_instance_id = self._run_instance(**kwargs)
         instance_uuid = ec2utils.ec2_inst_id_to_uuid(self.context,
                                                      ec2_instance_id)
-        vols = self.volume_api.get_all(self.context)
-        vols = [v for v in vols if v['instance_uuid'] == instance_uuid]
+        vols_dict = self.volume_api.get_all(self.context)
+        vols = []
+        for v in vols_dict:
+            if v['attachments'] != []:
+                if v['attachments'][0].get('instance_uuid') == instance_uuid:
+                    vols.append(v)
 
         self.assertEqual(len(vols), 2)
         for vol in vols:
@@ -864,20 +892,24 @@ class CinderCloudTestCase(test.TestCase):
         vol = self.volume_api.get(self.context, vol1_uuid)
         self._assert_volume_attached(vol, instance_uuid, '/dev/sdb')
 
-        vol = self.volume_api.get(self.context, vol1_uuid)
-        self._assert_volume_attached(vol, instance_uuid, '/dev/sdb')
-
         vol = self.volume_api.get(self.context, vol2_uuid)
         self._assert_volume_attached(vol, instance_uuid, '/dev/sdc')
 
         self.cloud.start_instances(self.context, [ec2_instance_id])
-        vols = self.volume_api.get_all(self.context)
-        vols = [v for v in vols if v['instance_uuid'] == instance_uuid]
+        vols_dict = self.volume_api.get_all(self.context)
+        vols = []
+        for v in vols_dict:
+            if v['attachments'] != []:
+                if v['attachments'][0].get('instance_uuid') == instance_uuid:
+                    vols.append(v)
+
         self.assertEqual(len(vols), 2)
         for vol in vols:
             self.assertIn(str(vol['id']), [str(vol1_uuid), str(vol2_uuid)])
-            self.assertIn(vol['mountpoint'], ['/dev/sdb', '/dev/sdc'])
-            self.assertEqual(vol['instance_uuid'], instance_uuid)
+            self.assertIn(vol['attachments'][0].get('mountpoint'),
+                          ['/dev/sdb', '/dev/sdc'])
+            self.assertEqual(vol['attachments'][0].get('instance_uuid'),
+                             instance_uuid)
             self.assertEqual(vol['status'], "in-use")
             self.assertEqual(vol['attach_status'], "attached")
 
@@ -915,8 +947,13 @@ class CinderCloudTestCase(test.TestCase):
         instance_uuid = ec2utils.ec2_inst_id_to_uuid(self.context,
                                                      ec2_instance_id)
 
-        vols = self.volume_api.get_all(self.context)
-        vols = [v for v in vols if v['instance_uuid'] == instance_uuid]
+        vols_dict = self.volume_api.get_all(self.context)
+        vols = []
+        for v in vols_dict:
+            if v['attachments'] != []:
+                if v['attachments'][0].get('instance_uuid') == instance_uuid:
+                    vols.append(v)
+
         self.assertEqual(len(vols), 1)
         for vol in vols:
             self.assertEqual(vol['id'], vol1_uuid)
@@ -949,8 +986,12 @@ class CinderCloudTestCase(test.TestCase):
         self._assert_volume_attached(vol2, instance_uuid, '/dev/sdc')
 
         self.cloud.start_instances(self.context, [ec2_instance_id])
-        vols = self.volume_api.get_all(self.context)
-        vols = [v for v in vols if v['instance_uuid'] == instance_uuid]
+        vols_dict = self.volume_api.get_all(self.context)
+        vols = []
+        for v in vols_dict:
+            if v['attachments'] != []:
+                if v['attachments'][0].get('instance_uuid') == instance_uuid:
+                    vols.append(v)
         self.assertEqual(len(vols), 1)
 
         self._assert_volume_detached(vol1)
@@ -999,8 +1040,12 @@ class CinderCloudTestCase(test.TestCase):
         instance_uuid = ec2utils.ec2_inst_id_to_uuid(self.context,
                                                      ec2_instance_id)
 
-        vols = self.volume_api.get_all(self.context)
-        vols = [v for v in vols if v['instance_uuid'] == instance_uuid]
+        vols_dict = self.volume_api.get_all(self.context)
+        vols = []
+        for v in vols_dict:
+            if v['attachments'] != []:
+                if v['attachments'][0].get('instance_uuid') == instance_uuid:
+                    vols.append(v)
 
         self.assertEqual(len(vols), 2)
 
diff --git a/nova/tests/compute/test_compute.py b/nova/tests/compute/test_compute.py
index 015b7d6..f4528a5 100644
--- a/nova/tests/compute/test_compute.py
+++ b/nova/tests/compute/test_compute.py
@@ -394,7 +394,9 @@ class ComputeVolumeTestCase(BaseTestCase):
                 fake_instance.fake_db_instance())
         self.stubs.Set(self.compute.volume_api, 'get', lambda *a, **kw:
                        {'id': self.volume_id,
-                        'attach_status': 'detached'})
+                        'attach_status': 'detached',
+                        'attachments': [{'attachment_id':
+                            "a26887c6-c47b-4654-abb5-dfadf7d3f803"}]})
         self.stubs.Set(self.compute.driver, 'get_volume_connector',
                        lambda *a, **kw: None)
         self.stubs.Set(self.compute.volume_api, 'initialize_connection',
@@ -426,6 +428,9 @@ class ComputeVolumeTestCase(BaseTestCase):
     def test_attach_volume_serial(self):
         fake_bdm = objects.BlockDeviceMapping(context=self.context,
                                               **self.fake_volume)
+        self.stubs.Set(objects.BlockDeviceMapping,
+                       'get_by_instance_and_volume_id',
+                       classmethod(lambda *a, **kw: fake_bdm))
         with (mock.patch.object(cinder.API, 'get_volume_encryption_metadata',
                                 return_value={})):
             instance = self._create_fake_instance()
@@ -440,6 +445,10 @@ class ComputeVolumeTestCase(BaseTestCase):
         def fake_attach(*args, **kwargs):
             raise test.TestingException
 
+        self.stubs.Set(objects.BlockDeviceMapping,
+                       'get_by_instance_and_volume_id',
+                       classmethod(lambda *a, **kw: fake_bdm))
+
         with contextlib.nested(
             mock.patch.object(driver_block_device.DriverVolumeBlockDevice,
                               'attach'),
@@ -463,7 +472,7 @@ class ComputeVolumeTestCase(BaseTestCase):
             mock.patch.object(self.compute, '_detach_volume'),
             mock.patch.object(self.compute.volume_api, 'detach'),
             mock.patch.object(objects.BlockDeviceMapping,
-                              'get_by_volume_id'),
+                              'get_by_instance_and_volume_id'),
             mock.patch.object(fake_bdm, 'destroy')
         ) as (mock_internal_detach, mock_detach, mock_get, mock_destroy):
             mock_detach.side_effect = test.TestingException
@@ -482,12 +491,13 @@ class ComputeVolumeTestCase(BaseTestCase):
 
         with contextlib.nested(
             mock.patch.object(objects.BlockDeviceMapping,
-                'get_by_volume_id', return_value=fake_bdm),
+                'get_by_instance_and_volume_id', return_value=fake_bdm),
             mock.patch.object(self.compute, '_attach_volume')
         ) as (mock_get_by_id, mock_attach):
             self.compute.attach_volume(self.context, 'fake', '/dev/vdb',
                     instance, bdm=None)
-            mock_get_by_id.assert_called_once_with(self.context, 'fake')
+            mock_get_by_id.assert_called_once_with(self.context, 'fake',
+                                                   instance['uuid'])
             self.assertTrue(mock_attach.called)
 
     def test_await_block_device_created_too_slow(self):
@@ -771,23 +781,30 @@ class ComputeVolumeTestCase(BaseTestCase):
     def test_detach_volume_usage(self):
         # Test that detach volume update the volume usage cache table correctly
         instance = self._create_fake_instance()
-        bdm = fake_block_device.FakeDbBlockDeviceDict(
-                {'id': 1, 'device_name': '/dev/vdb',
-                 'connection_info': '{}', 'instance_uuid': instance['uuid'],
-                 'source_type': 'volume', 'destination_type': 'volume',
-                 'volume_id': 1})
+        bdm = objects.BlockDeviceMapping(context=self.context,
+                                         id=1, device_name='/dev/vdb',
+                                         connection_info='{}',
+                                         instance_uuid=instance['uuid'],
+                                         source_type='volume',
+                                         destination_type='volume',
+                                         no_device=False,
+                                         disk_bus='foo',
+                                         device_type='disk',
+                                         volume_id=1)
         host_volume_bdms = {'id': 1, 'device_name': '/dev/vdb',
                'connection_info': '{}', 'instance_uuid': instance['uuid'],
                'volume_id': 1}
 
-        self.mox.StubOutWithMock(db, 'block_device_mapping_get_by_volume_id')
+        self.mox.StubOutWithMock(objects.BlockDeviceMapping,
+                                 'get_by_instance_and_volume_id')
         self.mox.StubOutWithMock(self.compute.driver, 'block_stats')
         self.mox.StubOutWithMock(self.compute, '_get_host_volume_bdms')
         self.mox.StubOutWithMock(self.compute.driver, 'get_all_volume_usage')
+        self.mox.StubOutWithMock(self.compute.driver, 'instance_exists')
 
         # The following methods will be called
-        db.block_device_mapping_get_by_volume_id(self.context, 1, []).\
-            AndReturn(bdm)
+        objects.BlockDeviceMapping.get_by_instance_and_volume_id(
+            self.context, 1, instance['uuid']).AndReturn(bdm.obj_clone())
         self.compute.driver.block_stats(instance['name'], 'vdb').\
             AndReturn([1L, 30L, 1L, 20L, None])
         self.compute._get_host_volume_bdms(self.context,
@@ -801,9 +818,13 @@ class ComputeVolumeTestCase(BaseTestCase):
                           'wr_req': 1,
                           'wr_bytes': 5,
                           'instance': instance}])
-        db.block_device_mapping_get_by_volume_id(self.context, 1, []).\
-            AndReturn(bdm)
 
+        self.compute.driver.instance_exists(mox.IgnoreArg()).AndReturn(True)
+
+        def fake_volume_attachment_get(self, volume, instance_uuid):
+            return {'attachment_id': 'abc123'}
+        self.stubs.Set(cinder.API, "get_volume_attachment",
+                       fake_volume_attachment_get)
         self.mox.ReplayAll()
 
         def fake_get_volume_encryption_metadata(self, context, volume_id):
@@ -811,7 +832,8 @@ class ComputeVolumeTestCase(BaseTestCase):
         self.stubs.Set(cinder.API, 'get_volume_encryption_metadata',
                        fake_get_volume_encryption_metadata)
 
-        self.compute.attach_volume(self.context, 1, '/dev/vdb', instance)
+        self.compute.attach_volume(self.context, 1, '/dev/vdb',
+                                   instance, bdm=bdm)
 
         # Poll volume usage & then detach the volume. This will update the
         # total fields in the volume usage cache.
@@ -1085,7 +1107,9 @@ class ComputeVolumeTestCase(BaseTestCase):
             def fake_volume_get(self, ctxt, volume_id):
                 return {'id': volume_id,
                         'status': status,
-                        'attach_status': attach_status}
+                        'attach_status': attach_status,
+                        'multiattach': False,
+                        'attachments': []}
             self.stubs.Set(cinder.API, 'get', fake_volume_get)
             self.assertRaises(exception.InvalidVolume,
                               self.compute_api._validate_bdm,
@@ -1107,7 +1131,9 @@ class ComputeVolumeTestCase(BaseTestCase):
         def fake_volume_get_ok(self, context, volume_id):
             return {'id': volume_id,
                     'status': 'available',
-                    'attach_status': 'detached'}
+                    'attach_status': 'detached',
+                    'multiattach': 'false',
+                    'attachments': []}
         self.stubs.Set(cinder.API, 'get', fake_volume_get_ok)
 
         self.compute_api._validate_bdm(self.context, self.instance,
@@ -1868,12 +1894,15 @@ class ComputeTestCase(BaseTestCase):
             pass
 
         def fake_volume_get(self, context, volume_id):
-            return {'id': volume_id}
+            return {'id': volume_id,
+                    'attachments': [{'attachment_id': 'abc123',
+                                    'instance_uuid': instance['uuid'],
+                                    'volume_id': volume_id}]}
 
         def fake_terminate_connection(self, context, volume_id, connector):
             pass
 
-        def fake_detach(self, context, volume_id):
+        def fake_detach(self, context, volume_id, attachment_id):
             pass
 
         bdms = []
@@ -9143,9 +9172,9 @@ class ComputeAPITestCase(BaseTestCase):
                 {'source_type': 'volume', 'destination_type': 'volume',
                  'volume_id': 'fake-volume-id', 'device_name': '/dev/vdb'})
         bdm = block_device_obj.BlockDeviceMapping()._from_db_object(
-                self.context,
-                block_device_obj.BlockDeviceMapping(),
-                fake_bdm)
+            self.context,
+            block_device_obj.BlockDeviceMapping(),
+            fake_bdm)
         instance = self._create_fake_instance()
         fake_volume = {'id': 'fake-volume-id'}
 
@@ -9225,8 +9254,11 @@ class ComputeAPITestCase(BaseTestCase):
         # Ensure volume can be detached from instance
         called = {}
         instance = self._create_fake_instance()
-        volume = {'id': 1, 'attach_status': 'in-use',
-                  'instance_uuid': instance['uuid']}
+        volume = {'id': 1,
+                  'attach_status': 'in-use',
+                  'attachments': [{
+                      'attachment_id': 'abc123',
+                      'instance_uuid': instance['uuid']}]}
 
         def fake_check_detach(*args, **kwargs):
             called['fake_check_detach'] = True
@@ -9269,8 +9301,8 @@ class ComputeAPITestCase(BaseTestCase):
                     'launched_at': timeutils.utcnow(),
                     'vm_state': vm_states.ACTIVE,
                     'task_state': None}
-        volume = {'id': 1, 'attach_status': 'in-use',
-                  'instance_uuid': 'uuid2'}
+        volume = {'id': 1, 'attach_status': 'in-use', 'status': 'available',
+                  'attachments': []}
 
         self.assertRaises(exception.VolumeUnattached,
                           self.compute_api.detach_volume, self.context,
@@ -9317,11 +9349,12 @@ class ComputeAPITestCase(BaseTestCase):
         self.stubs.Set(self.compute.driver, "detach_volume",
                        fake_libvirt_driver_detach_volume_fails)
 
-        self.mox.StubOutWithMock(objects.BlockDeviceMapping,
-                                 'get_by_volume_id')
-        objects.BlockDeviceMapping.get_by_volume_id(
-                self.context, 1).AndReturn(objects.BlockDeviceMapping(
-                    context=self.context, **fake_bdm))
+        self.mox.StubOutWithMock(block_device_obj.BlockDeviceMapping,
+                                 'get_by_instance_and_volume_id')
+        objects.BlockDeviceMapping.get_by_instance_and_volume_id(
+                self.context, 1, instance['uuid']).AndReturn(
+                        objects.BlockDeviceMapping(
+                            context=self.context, **fake_bdm))
         self.mox.ReplayAll()
 
         self.assertRaises(AttributeError, self.compute.detach_volume,
@@ -9346,10 +9379,15 @@ class ComputeAPITestCase(BaseTestCase):
             return {'id': volume_id}
         self.stubs.Set(cinder.API, "get", fake_volume_get)
 
+        def fake_volume_attachment_get(self, volume, instance_uuid):
+            return {'attachment_id': 'abc123'}
+        self.stubs.Set(cinder.API, "get_volume_attachment",
+                       fake_volume_attachment_get)
+
         # Stub out and record whether it gets detached
         result = {"detached": False}
 
-        def fake_detach(self, context, volume_id_param):
+        def fake_detach(self, context, volume_id_param, attachment_id):
             result["detached"] = volume_id_param == volume_id
         self.stubs.Set(cinder.API, "detach", fake_detach)
 
@@ -9389,7 +9427,19 @@ class ComputeAPITestCase(BaseTestCase):
             bdm_obj.create(admin)
             bdms.append(bdm_obj)
 
-        self.stubs.Set(self.compute, 'volume_api', mox.MockAnything())
+        self.stubs.Set(cinder.API, "terminate_connection", mox.MockAnything())
+        self.stubs.Set(cinder.API, "detach", mox.MockAnything())
+
+        def fake_volume_get(self, context, volume_id):
+            return {'id': volume_id}
+
+        self.stubs.Set(cinder.API, "get", fake_volume_get)
+
+        def fake_volume_attachment_get(self, volume, instance_uuid):
+            return {'attachment_id': '123'}
+        self.stubs.Set(cinder.API, "get_volume_attachment",
+                       fake_volume_attachment_get)
+
         self.stubs.Set(self.compute, '_prep_block_device', mox.MockAnything())
         self.compute.run_instance(self.context, instance, {}, {}, None, None,
                 None, True, None, False)
@@ -10498,8 +10548,7 @@ class DisabledInstanceTypesTestCase(BaseTestCase):
 
     def test_can_resize_to_visible_instance_type(self):
         instance = self._create_fake_instance_obj()
-        orig_get_flavor_by_flavor_id =\
-                flavors.get_flavor_by_flavor_id
+        orig_get_flavor_by_flavor_id = flavors.get_flavor_by_flavor_id
 
         def fake_get_flavor_by_flavor_id(flavor_id, ctxt=None,
                                                 read_deleted="yes"):
@@ -10517,8 +10566,7 @@ class DisabledInstanceTypesTestCase(BaseTestCase):
 
     def test_cannot_resize_to_disabled_instance_type(self):
         instance = self._create_fake_instance_obj()
-        orig_get_flavor_by_flavor_id = \
-                flavors.get_flavor_by_flavor_id
+        orig_get_flavor_by_flavor_id = flavors.get_flavor_by_flavor_id
 
         def fake_get_flavor_by_flavor_id(flavor_id, ctxt=None,
                                                 read_deleted="yes"):
@@ -11117,10 +11165,17 @@ class EvacuateHostTestCase(BaseTestCase):
             return {'id': 'fake_volume_id'}
         self.stubs.Set(cinder.API, "get", fake_volume_get)
 
+        def fake_get_volume_attachment(self, volume, instance_uuid):
+            return {'instance_uuid': instance_uuid,
+                    'mountpoint': '/dev/vdc',
+                    'attachment_id': 'abc123'}
+        self.stubs.Set(cinder.API, "get_volume_attachment",
+                       fake_get_volume_attachment)
+
         # Stub out and record whether it gets detached
         result = {"detached": False}
 
-        def fake_detach(self, context, volume):
+        def fake_detach(self, context, volume, attachment_id):
             result["detached"] = volume["id"] == 'fake_volume_id'
         self.stubs.Set(cinder.API, "detach", fake_detach)
 
@@ -11136,7 +11191,8 @@ class EvacuateHostTestCase(BaseTestCase):
 
         # make sure volumes attach, detach are called
         self.mox.StubOutWithMock(self.compute.volume_api, 'detach')
-        self.compute.volume_api.detach(mox.IsA(self.context), mox.IgnoreArg())
+        self.compute.volume_api.detach(mox.IsA(self.context), mox.IgnoreArg(),
+                                       mox.IgnoreArg())
 
         self.mox.StubOutWithMock(self.compute, '_prep_block_device')
         self.compute._prep_block_device(mox.IsA(self.context),
diff --git a/nova/tests/compute/test_compute_api.py b/nova/tests/compute/test_compute_api.py
index ae7bad5..8888266 100644
--- a/nova/tests/compute/test_compute_api.py
+++ b/nova/tests/compute/test_compute_api.py
@@ -1424,14 +1424,16 @@ class _ComputeAPIUnitTestMixIn(object):
                                   'attach_status': 'attached',
                                   'instance_uuid': 'fake',
                                   'size': 5,
-                                  'status': 'in-use'}
+                                  'status': 'in-use',
+                                  'multiattach': 'false'}
         new_volume_id = uuidutils.generate_uuid()
         volumes[new_volume_id] = {'id': new_volume_id,
                                   'display_name': 'new_volume',
                                   'attach_status': 'detached',
                                   'instance_uuid': None,
                                   'size': 5,
-                                  'status': 'available'}
+                                  'status': 'available',
+                                  'multiattach': 'false'}
         self.assertRaises(exception.InstanceInvalidState,
                           self.compute_api.swap_volume, self.context, instance,
                           volumes[old_volume_id], volumes[new_volume_id])
diff --git a/nova/tests/compute/test_compute_mgr.py b/nova/tests/compute/test_compute_mgr.py
index 91c7901..49ecf27 100644
--- a/nova/tests/compute/test_compute_mgr.py
+++ b/nova/tests/compute/test_compute_mgr.py
@@ -1085,22 +1085,26 @@ class ComputeManagerUnitTestCase(test.NoDBTestCase):
         volumes[old_volume_id] = {'id': old_volume_id,
                                   'display_name': 'old_volume',
                                   'status': 'detaching',
-                                  'size': 1}
+                                  'size': 1,
+                                  'attachments': [{'attachment_id': 'abc123',
+                                                   'volume_id': old_volume_id,
+                                                   'instance_uuid': 'fake'}]}
         new_volume_id = uuidutils.generate_uuid()
         volumes[new_volume_id] = {'id': new_volume_id,
                                   'display_name': 'new_volume',
                                   'status': 'available',
-                                  'size': 2}
+                                  'size': 2,
+                                  'attachments': []}
 
         def fake_vol_api_roll_detaching(context, volume_id):
             self.assertTrue(uuidutils.is_uuid_like(volume_id))
             if volumes[volume_id]['status'] == 'detaching':
                 volumes[volume_id]['status'] = 'in-use'
 
-        fake_bdm = fake_block_device.FakeDbBlockDeviceDict(
-                   {'device_name': '/dev/vdb', 'source_type': 'volume',
-                    'destination_type': 'volume', 'instance_uuid': 'fake',
-                    'connection_info': '{"foo": "bar"}'})
+        fake_bdm_dict = fake_block_device.FakeDbBlockDeviceDict(
+            {'device_name': '/dev/vdb', 'source_type': 'volume',
+             'destination_type': 'volume', 'instance_uuid': 'fake',
+             'connection_info': '{"foo": "bar"}'})
 
         def fake_vol_api_func(context, volume, *args):
             self.assertTrue(uuidutils.is_uuid_like(volume))
@@ -1138,8 +1142,9 @@ class ComputeManagerUnitTestCase(test.NoDBTestCase):
                        fake_vol_unreserve)
         self.stubs.Set(self.compute.volume_api, 'terminate_connection',
                        fake_vol_api_func)
-        self.stubs.Set(db, 'block_device_mapping_get_by_volume_id',
-                       lambda x, y, z: fake_bdm)
+        self.stubs.Set(db,
+                       'block_device_mapping_get_by_instance_and_volume_id',
+                       lambda *a, **k: fake_bdm_dict)
         self.stubs.Set(self.compute.driver, 'get_volume_connector',
                        lambda x: {})
         self.stubs.Set(self.compute.driver, 'swap_volume',
@@ -1147,7 +1152,7 @@ class ComputeManagerUnitTestCase(test.NoDBTestCase):
         self.stubs.Set(self.compute.volume_api, 'migrate_volume_completion',
                       fake_vol_migrate_volume_completion)
         self.stubs.Set(db, 'block_device_mapping_update',
-                       lambda *a, **k: fake_bdm)
+                       lambda *a, **k: fake_bdm_dict)
         self.stubs.Set(db,
                        'instance_fault_create',
                        lambda x, y:
@@ -1441,18 +1446,21 @@ class ComputeManagerUnitTestCase(test.NoDBTestCase):
             self.assertFalse(allow_reboot)
             self.assertEqual(reboot_type, 'HARD')
 
-    @mock.patch('nova.objects.BlockDeviceMapping.get_by_volume_id')
+    @mock.patch('nova.objects.BlockDeviceMapping.'
+                'get_by_instance_and_volume_id')
     @mock.patch('nova.compute.manager.ComputeManager._detach_volume')
     @mock.patch('nova.objects.Instance._from_db_object')
     def test_remove_volume_connection(self, inst_from_db, detach, bdm_get):
         bdm = mock.sentinel.bdm
-        inst_obj = mock.sentinel.inst_obj
+        inst_obj = mock.Mock()
+        inst_obj.uuid = 'uuid'
         bdm_get.return_value = bdm
         inst_from_db.return_value = inst_obj
         with mock.patch.object(self.compute, 'volume_api'):
             self.compute.remove_volume_connection(self.context, 'vol',
                                                   inst_obj)
         detach.assert_called_once_with(self.context, inst_obj, bdm)
+        bdm_get.assert_called_once_with(self.context, 'vol', 'uuid')
 
     def _test_rescue(self, clean_shutdown=True):
         instance = fake_instance.fake_instance_obj(
diff --git a/nova/tests/fake_volume.py b/nova/tests/fake_volume.py
index 6def42f..30e5145 100644
--- a/nova/tests/fake_volume.py
+++ b/nova/tests/fake_volume.py
@@ -67,7 +67,9 @@ class fake_volume():
             'display_description': description,
             'provider_location': 'fake-location',
             'provider_auth': 'fake-auth',
-            'volume_type_id': 99
+            'volume_type_id': 99,
+            'attachments': [],
+            'multiattach': False
             }
 
     def get(self, key, default=None):
@@ -204,10 +206,12 @@ class API(object):
         LOG.info('attaching volume %s', volume_id)
         volume = self.get(context, volume_id)
         volume['status'] = 'in-use'
-        volume['mountpoint'] = mountpoint
         volume['attach_status'] = 'attached'
-        volume['instance_uuid'] = instance_uuid
         volume['attach_time'] = timeutils.utcnow()
+        attachment = {'attachment_id': str(uuid.uuid4()),
+                      'mountpoint': mountpoint,
+                      'instance_uuid': instance_uuid}
+        volume['attachments'] = [attachment]
 
     def fake_set_snapshot_id(self, context, volume, snapshot_id):
         volume['snapshot_id'] = snapshot_id
@@ -216,13 +220,21 @@ class API(object):
         del self.volume_list[:]
         del self.snapshot_list[:]
 
-    def detach(self, context, volume_id):
+    def detach(self, context, volume_id, attachment_id):
         LOG.info('detaching volume %s', volume_id)
         volume = self.get(context, volume_id)
         volume['status'] = 'available'
-        volume['mountpoint'] = None
         volume['attach_status'] = 'detached'
-        volume['instance_uuid'] = None
+        volume['attachments'] = []
+
+    def get_volume_attachment(self, volume, instance_uuid):
+        attachments = volume['attachments']
+        attachment = None
+        for attach in attachments:
+            if attach['instance_uuid'] == instance_uuid:
+                attachment = attach
+                break
+        return attachment
 
     def initialize_connection(self, context, volume_id, connector):
         return {'driver_volume_type': 'iscsi', 'data': {}}
diff --git a/nova/tests/volume/test_cinder.py b/nova/tests/volume/test_cinder.py
index b33c05c..9f14c3e 100644
--- a/nova/tests/volume/test_cinder.py
+++ b/nova/tests/volume/test_cinder.py
@@ -19,6 +19,8 @@ import mock
 from nova import context
 from nova import exception
 from nova import test
+from nova.tests.fake_instance import fake_instance_obj
+from nova.tests.fake_volume import fake_volume
 from nova.volume import cinder
 
 
@@ -55,6 +57,8 @@ class FakeVolume(object):
         self.volume_type_id = dict.get('volume_type_id')
         self.snapshot_id = dict.get('snapshot_id')
         self.metadata = dict.get('volume_metadata') or {}
+        self.multiattach = dict.get('multiattach') or False
+        self.attachments = dict.get('attachments') or []
 
 
 class CinderApiTestCase(test.NoDBTestCase):
@@ -129,18 +133,36 @@ class CinderApiTestCase(test.NoDBTestCase):
         self.assertEqual(['id1', 'id2'], self.api.get_all(self.ctx))
 
     def test_check_attach_volume_status_error(self):
-        volume = {'id': 'fake', 'status': 'error'}
+        volume = {'id': 'fake', 'status': 'error', 'multiattach': False}
+        self.assertRaises(exception.InvalidVolume,
+                          self.api.check_attach, self.ctx, volume)
+        volume = {'id': 'fake', 'status': 'error', 'multiattach': True}
         self.assertRaises(exception.InvalidVolume,
                           self.api.check_attach, self.ctx, volume)
 
     def test_check_attach_volume_already_attached(self):
-        volume = {'id': 'fake', 'status': 'available'}
+        volume = {'id': 'fake', 'status': 'available',
+                  'multiattach': False}
         volume['attach_status'] = "attached"
         self.assertRaises(exception.InvalidVolume,
                           self.api.check_attach, self.ctx, volume)
 
+    def test_check_attach_multiattach_volume_already_attached(self):
+        instance = fake_instance_obj(self.ctx)
+        volume = fake_volume(size=1, name='', description='', volume_id=None,
+                             snapshot=None, volume_type=None, metadata={},
+                             availability_zone='cinder')
+        volume.multiattach = True
+        volume.status = 'in-use'
+        volume.attach_status = 'attached'
+        volume.attachments = [{'instance_uuid': instance.uuid,
+                               'attach_status': 'attached'}]
+        self.assertRaises(exception.InvalidVolume,
+                          self.api.check_attach, self.ctx, volume, instance)
+
     def test_check_attach_availability_zone_differs(self):
-        volume = {'id': 'fake', 'status': 'available'}
+        volume = {'id': 'fake', 'status': 'available',
+                  'multiattach': False}
         volume['attach_status'] = "detached"
         instance = {'id': 'fake',
                     'availability_zone': 'zone1', 'host': 'fakehost'}
@@ -175,6 +197,7 @@ class CinderApiTestCase(test.NoDBTestCase):
         volume = {'status': 'available'}
         volume['attach_status'] = "detached"
         volume['availability_zone'] = 'zone1'
+        volume['multiattach'] = False
         instance = {'availability_zone': 'zone1', 'host': 'fakehost'}
         cinder.CONF.set_override('cross_az_attach', False, group='cinder')
 
@@ -254,10 +277,10 @@ class CinderApiTestCase(test.NoDBTestCase):
         cinder.cinderclient(self.ctx).AndReturn(self.cinderclient)
         self.mox.StubOutWithMock(self.cinderclient.volumes,
                                  'detach')
-        self.cinderclient.volumes.detach('id1')
+        self.cinderclient.volumes.detach('id1', 'abc123')
         self.mox.ReplayAll()
 
-        self.api.detach(self.ctx, 'id1')
+        self.api.detach(self.ctx, 'id1', 'abc123')
 
     def test_initialize_connection(self):
         cinder.cinderclient(self.ctx).AndReturn(self.cinderclient)
diff --git a/nova/volume/cinder.py b/nova/volume/cinder.py
index 66cc406..0dccd99 100644
--- a/nova/volume/cinder.py
+++ b/nova/volume/cinder.py
@@ -27,6 +27,7 @@ from cinderclient.v1 import client as v1_client
 from keystoneclient import exceptions as keystone_exception
 from keystoneclient import session
 from oslo.config import cfg
+import six
 import six.moves.urllib.parse as urlparse
 
 from nova import availability_zones as az
@@ -38,17 +39,13 @@ from nova.openstack.common import strutils
 
 cinder_opts = [
     cfg.StrOpt('catalog_info',
-            default='volume:cinder:publicURL',
+            default='volumev2:cinderv2:publicURL',
             help='Info to match when looking for cinder in the service '
                  'catalog. Format is: separated values of the form: '
-                 '<service_type>:<service_name>:<endpoint_type>',
-            deprecated_group='DEFAULT',
-            deprecated_name='cinder_catalog_info'),
+                 '<service_type>:<service_name>:<endpoint_type>'),
     cfg.StrOpt('endpoint_template',
                help='Override service catalog lookup with template for cinder '
-                    'endpoint e.g. http://localhost:8776/v1/%(project_id)s',
-               deprecated_group='DEFAULT',
-               deprecated_name='cinder_endpoint_template'),
+                    'endpoint e.g. http://localhost:8776/v1/%(project_id)s'),
     cfg.StrOpt('os_region_name',
                help='Region name of this node'),
     cfg.IntOpt('http_retries',
@@ -57,9 +54,7 @@ cinder_opts = [
     cfg.BoolOpt('cross_az_attach',
                 default=True,
                 help='Allow attach between instance and volume in different '
-                     'availability zones.',
-                deprecated_group='DEFAULT',
-                deprecated_name='cinder_cross_az_attach'),
+                     'availability zones.'),
 ]
 
 CONF = cfg.CONF
@@ -103,7 +98,6 @@ def cinderclient(context):
 
     url = None
     endpoint_override = None
-    version = None
 
     auth = context.get_auth_plugin()
     service_type, service_name, interface = CONF.cinder.catalog_info.split(':')
@@ -128,7 +122,7 @@ def cinderclient(context):
         msg = _LW('Cinder V1 API is deprecated as of the Juno '
                   'release, and Nova is still configured to use it. '
                   'Enable the V2 API in Cinder and set '
-                  'cinder_catalog_info in nova.conf to use it.')
+                  'cinder.catalog_info in nova.conf to use it.')
         LOG.warn(msg)
         _V1_ERROR_RAISED = True
 
@@ -155,12 +149,20 @@ def _untranslate_volume_summary_view(context, vol):
     #            removed.
     d['attach_time'] = ""
     d['mountpoint'] = ""
+    d['multiattach'] = getattr(vol, 'multiattach', False)
 
     if vol.attachments:
-        att = vol.attachments[0]
+        d['attachments'] = []
+        for attach in vol.attachments:
+            attachment_id = attach.get('attachment_id')
+            a = {'attach_status': 'attached',
+                 'attachment_id': attachment_id,
+                 'instance_uuid': attach['server_id'],
+                 'mountpoint': attach['device'],
+                 }
+            d['attachments'].append(a)
+
         d['attach_status'] = 'attached'
-        d['instance_uuid'] = att['server_id']
-        d['mountpoint'] = att['device']
     else:
         d['attach_status'] = 'detached'
     # NOTE(dzyu) volume(cinder) v2 API uses 'name' instead of 'display_name',
@@ -225,14 +227,15 @@ def translate_volume_exception(method):
                 exc_value = exception.VolumeNotFound(volume_id=volume_id)
             elif isinstance(exc_value, (keystone_exception.BadRequest,
                                         cinder_exception.BadRequest)):
-                exc_value = exception.InvalidInput(reason=exc_value.message)
-            raise exc_value, None, exc_trace
+                exc_value = exception.InvalidInput(
+                    reason=six.text_type(exc_value))
+            six.reraise(exc_value, None, exc_trace)
         except (cinder_exception.ConnectionError,
                 keystone_exception.ConnectionError):
             exc_type, exc_value, exc_trace = sys.exc_info()
             exc_value = exception.CinderConnectionFailed(
-                                                   reason=exc_value.message)
-            raise exc_value, None, exc_trace
+                reason=six.text_type(exc_value))
+            six.reraise(exc_value, None, exc_trace)
         return res
     return wrapper
 
@@ -250,13 +253,13 @@ def translate_snapshot_exception(method):
             if isinstance(exc_value, (keystone_exception.NotFound,
                                       cinder_exception.NotFound)):
                 exc_value = exception.SnapshotNotFound(snapshot_id=snapshot_id)
-            raise exc_value, None, exc_trace
+            six.reraise(exc_value, None, exc_trace)
         except (cinder_exception.ConnectionError,
                 keystone_exception.ConnectionError):
             exc_type, exc_value, exc_trace = sys.exc_info()
-            exc_value = exception.CinderConnectionFailed(
-                                                  reason=exc_value.message)
-            raise exc_value, None, exc_trace
+            reason = six.text_type(exc_value)
+            exc_value = exception.CinderConnectionFailed(reason=reason)
+            six.reraise(exc_value, None, exc_trace)
         return res
     return wrapper
 
@@ -308,15 +311,32 @@ class API(object):
             raise exception.InvalidVolume(reason=msg)
 
     def check_attach(self, context, volume, instance=None):
+
+        def get_instances_by_vol_attachments(volume):
+            return (attachment['instance_uuid']
+                    for attachment in volume['attachments']
+                    if attachment['attach_status'] == 'attached')
+
         # TODO(vish): abstract status checking?
-        if volume['status'] != "available":
-            msg = _("volume '%(vol)s' status must be 'available'. Currently "
-                    "in '%(status)s'") % {'vol': volume['id'],
-                                          'status': volume['status']}
-            raise exception.InvalidVolume(reason=msg)
-        if volume['attach_status'] == "attached":
-            msg = _("volume %s already attached") % volume['id']
-            raise exception.InvalidVolume(reason=msg)
+        # Cinder supports multi-attach when a volume
+        # has been marked as multiattach
+        if volume.get('multiattach', False):
+            if (volume['status'] not in ("available", 'in-use')):
+                msg = _("multiattach volume's status must be"
+                        "'available' or 'in-use'")
+                raise exception.InvalidVolume(reason=msg)
+            if instance and volume.get('attachments'):
+                if instance.uuid in get_instances_by_vol_attachments(volume):
+                    msg = _("volume is already attached")
+                    raise exception.InvalidVolume(reason=msg)
+        else:
+            if volume['status'] != "available":
+                msg = _("status must be 'available'")
+                raise exception.InvalidVolume(reason=msg)
+            if volume['attach_status'] == "attached":
+                msg = _("volume is already attached")
+                raise exception.InvalidVolume(reason=msg)
+
         if instance and not CONF.cinder.cross_az_attach:
             # NOTE(sorrison): If instance is on a host we match against it's AZ
             #                 else we check the intended AZ
@@ -363,8 +383,20 @@ class API(object):
                                              mountpoint, mode=mode)
 
     @translate_volume_exception
-    def detach(self, context, volume_id):
-        cinderclient(context).volumes.detach(volume_id)
+    def get_volume_attachment(self, volume, instance_uuid):
+        if 'attachments' in volume:
+            attachments = volume['attachments']
+        else:
+            attachments = {}
+            attach = {}
+        for attach in attachments:
+            if attach.get('instance_uuid', None) == instance_uuid:
+                break
+        return attach
+
+    @translate_volume_exception
+    def detach(self, context, volume_id, attachment_id):
+        cinderclient(context).volumes.detach(volume_id, attachment_id)
 
     @translate_volume_exception
     def initialize_connection(self, context, volume_id, connector):
-- 
2.4.3

