From 0ba4ab2944b698065a7cb426fde26981b3a6ae12 Mon Sep 17 00:00:00 2001
From: "Walter A. Boring IV" <walter.boring@hp.com>
Date: Wed, 10 Dec 2014 01:03:39 +0000
Subject: [PATCH 12/13] Add volume multi attach support

This patch includes the Cinder changes needed
to support volume multiple attaches.  Nova and
python-cinderclient also need patches associated
to provide support for multiple attachments.

This adds the multiattach flag to volumes.  When a
volume is created, a multiattach flag can be set,
which allows a volume to be attached to more than
one Nova instance or host.  If the multiattach flag is
not set on a volume, it cannot be attached to more
than one Nova instance or host

Each volume attachment is tracked in a
new volume_attachment table.  The attachment id is
the unique identifier for each attachment to an
instance or host.

When a volume is to be detached the attachment
uuid must be passed in to the detach call in
order to determine which attachment should be
removed.  Since a volume can be attached to an
instance and a host, the attachment id is used
as the attachment identifier.

Nova:
https://review.openstack.org/#/c/153033/
https://review.openstack.org/#/c/153038/

python-cinderclient:
https://review.openstack.org/#/c/85856/

Change-Id: I950fa00ed5a30e7758245d5b0557f6df42dc58a3
Implements: blueprint multi-attach-volume
APIImpact

(cherry picked from commit 10d54216871fde27172fbcb1a3c9bfec59b824c2)

Conflicts:
	cinder/api/v1/volumes.py
	cinder/backup/manager.py
	cinder/db/sqlalchemy/api.py
	cinder/objects/volume.py
	cinder/tests/api/v2/test_volumes.py
	cinder/tests/test_hp3par.py
	cinder/tests/test_migrations.py
	cinder/tests/test_volume.py
	cinder/volume/api.py
	cinder/volume/drivers/san/hp/hp_3par_common.py
	cinder/volume/drivers/san/hp/hp_3par_fc.py
	cinder/volume/drivers/san/hp/hp_3par_iscsi.py
	cinder/volume/flows/api/create_volume.py
	cinder/volume/manager.py
	cinder/volume/rpcapi.py
---
 cinder/api/contrib/admin_actions.py                |   5 +-
 cinder/api/contrib/volume_actions.py               |   6 +-
 cinder/api/v1/volumes.py                           |  38 +-
 cinder/api/v2/views/volumes.py                     |  27 +-
 cinder/api/v2/volumes.py                           |   4 +
 cinder/backup/manager.py                           |  37 +-
 cinder/db/api.py                                   |  35 +-
 cinder/db/sqlalchemy/api.py                        | 160 +++++-
 .../versions/028_add_volume_attachment.py          | 147 ++++++
 .../migrate_repo/versions/028_sqlite_downgrade.sql |  87 ++++
 cinder/db/sqlalchemy/models.py                     |  26 +-
 cinder/exception.py                                |   5 +
 cinder/tests/api/contrib/test_admin_actions.py     | 106 ++--
 cinder/tests/api/contrib/test_volume_actions.py    |  12 +-
 cinder/tests/api/v1/stubs.py                       |   5 +-
 cinder/tests/api/v1/test_volumes.py                | 128 ++---
 cinder/tests/api/v2/stubs.py                       |   8 +-
 cinder/tests/api/v2/test_volumes.py                | 278 ++++++++---
 cinder/tests/test_backup.py                        |  15 +-
 cinder/tests/test_db_api.py                        |  72 ++-
 cinder/tests/test_hp3par.py                        |  33 +-
 cinder/tests/test_migrations.py                    |  38 +-
 cinder/tests/test_volume.py                        | 543 +++++++++++++++++----
 cinder/tests/test_volume_rpcapi.py                 |   4 +-
 cinder/tests/utils.py                              |  17 +
 cinder/volume/api.py                               |  29 +-
 cinder/volume/driver.py                            |   2 +-
 cinder/volume/drivers/datera.py                    |   2 +-
 cinder/volume/drivers/san/hp/hp_3par_common.py     |  34 +-
 cinder/volume/drivers/san/hp/hp_3par_fc.py         |  13 +-
 cinder/volume/drivers/san/hp/hp_3par_iscsi.py      |  16 +-
 cinder/volume/drivers/scality.py                   |   2 +-
 cinder/volume/drivers/solidfire.py                 |   2 +-
 cinder/volume/flows/api/create_volume.py           |   4 +-
 cinder/volume/manager.py                           | 167 ++++---
 cinder/volume/rpcapi.py                            |  15 +-
 cinder/volume/utils.py                             |   1 -
 37 files changed, 1635 insertions(+), 488 deletions(-)
 create mode 100644 cinder/db/sqlalchemy/migrate_repo/versions/028_add_volume_attachment.py
 create mode 100644 cinder/db/sqlalchemy/migrate_repo/versions/028_sqlite_downgrade.sql

diff --git a/cinder/api/contrib/admin_actions.py b/cinder/api/contrib/admin_actions.py
index 16fe990..20dd687 100644
--- a/cinder/api/contrib/admin_actions.py
+++ b/cinder/api/contrib/admin_actions.py
@@ -185,7 +185,10 @@ class VolumeAdminController(AdminController):
             raise exc.HTTPNotFound()
         self.volume_api.terminate_connection(context, volume,
                                              {}, force=True)
-        self.volume_api.detach(context, volume)
+
+        attachment_id = body['os-force_detach'].get('attachment_id', None)
+
+        self.volume_api.detach(context, volume, attachment_id)
         return webob.Response(status_int=202)
 
     @wsgi.action('os-migrate_volume')
diff --git a/cinder/api/contrib/volume_actions.py b/cinder/api/contrib/volume_actions.py
index 24dcb9c..bedb14c 100644
--- a/cinder/api/contrib/volume_actions.py
+++ b/cinder/api/contrib/volume_actions.py
@@ -127,7 +127,11 @@ class VolumeActionsController(wsgi.Controller):
         except exception.VolumeNotFound as error:
             raise webob.exc.HTTPNotFound(explanation=error.msg)
 
-        self.volume_api.detach(context, volume)
+        attachment_id = None
+        if body['os-detach']:
+            attachment_id = body['os-detach'].get('attachment_id', None)
+
+        self.volume_api.detach(context, volume, attachment_id)
         return webob.Response(status_int=202)
 
     @wsgi.action('os-reserve')
diff --git a/cinder/api/v1/volumes.py b/cinder/api/v1/volumes.py
index 5e6968f..cc63f04 100644
--- a/cinder/api/v1/volumes.py
+++ b/cinder/api/v1/volumes.py
@@ -25,6 +25,7 @@ from cinder.api.openstack import wsgi
 from cinder.api import xmlutil
 from cinder import exception
 from cinder.i18n import _
+from cinder.openstack.common.gettextutils import _LE, _LI, _LW
 from cinder.openstack.common import log as logging
 from cinder.openstack.common import uuidutils
 from cinder import utils
@@ -48,18 +49,18 @@ def _translate_attachment_detail_view(_context, vol):
 
 def _translate_attachment_summary_view(_context, vol):
     """Maps keys for attachment summary view."""
-    d = {}
-
-    volume_id = vol['id']
-
-    # NOTE(justinsb): We use the volume id as the id of the attachment object
-    d['id'] = volume_id
-
-    d['volume_id'] = volume_id
-    d['server_id'] = vol['instance_uuid']
-    d['host_name'] = vol['attached_host']
-    if vol.get('mountpoint'):
-        d['device'] = vol['mountpoint']
+    d = []
+    attachments = vol.get('volume_attachment', [])
+    for attachment in attachments:
+        if attachment.get('attach_status') == 'attached':
+            a = {'id': attachment.get('volume_id'),
+                 'attachment_id': attachment.get('id'),
+                 'volume_id': attachment.get('volume_id'),
+                 'server_id': attachment.get('instance_uuid'),
+                 'host_name': attachment.get('attached_host'),
+                 'device': attachment.get('mountpoint'),
+                 }
+            d.append(a)
 
     return d
 
@@ -91,10 +92,14 @@ def _translate_volume_summary_view(context, vol, image_id=None):
     else:
         d['bootable'] = 'false'
 
+    if vol['multiattach']:
+        d['multiattach'] = 'true'
+    else:
+        d['multiattach'] = 'false'
+
     d['attachments'] = []
     if vol['attach_status'] == 'attached':
-        attachment = _translate_attachment_detail_view(context, vol)
-        d['attachments'].append(attachment)
+        d['attachments'] = _translate_attachment_detail_view(context, vol)
 
     d['display_name'] = vol['display_name']
     d['display_description'] = vol['display_description']
@@ -147,6 +152,7 @@ def make_volume(elem):
     elem.set('volume_type')
     elem.set('snapshot_id')
     elem.set('source_volid')
+    elem.set('multiattach')
 
     attachments = xmlutil.SubTemplateElement(elem, 'attachments')
     attachment = xmlutil.SubTemplateElement(attachments, 'attachment',
@@ -368,7 +374,9 @@ class VolumeController(wsgi.Controller):
         elif size is None and kwargs['source_volume'] is not None:
             size = kwargs['source_volume']['size']
 
-        LOG.info(_("Create volume of %s GB"), size, context=context)
+        LOG.info(_LI("Create volume of %s GB"), size, context=context)
+        multiattach = volume.get('multiattach', False)
+        kwargs['multiattach'] = multiattach
 
         image_href = None
         image_uuid = None
diff --git a/cinder/api/v2/views/volumes.py b/cinder/api/v2/views/volumes.py
index c67748d..77303b2 100644
--- a/cinder/api/v2/views/volumes.py
+++ b/cinder/api/v2/views/volumes.py
@@ -70,7 +70,8 @@ class ViewBuilder(common.ViewBuilder):
                 'bootable': str(volume.get('bootable')).lower(),
                 'encrypted': self._is_volume_encrypted(volume),
                 'replication_status': volume.get('replication_status'),
-                'consistencygroup_id': volume.get('consistencygroup_id')
+                'consistencygroup_id': volume.get('consistencygroup_id'),
+                'multiattach': volume.get('multiattach')
             }
         }
 
@@ -83,19 +84,17 @@ class ViewBuilder(common.ViewBuilder):
         attachments = []
 
         if volume['attach_status'] == 'attached':
-            d = {}
-            volume_id = volume['id']
-
-            # note(justinsb): we use the volume id as the id of the attachments
-            # object
-            d['id'] = volume_id
-
-            d['volume_id'] = volume_id
-            d['server_id'] = volume['instance_uuid']
-            d['host_name'] = volume['attached_host']
-            if volume.get('mountpoint'):
-                d['device'] = volume['mountpoint']
-            attachments.append(d)
+            attaches = volume.get('volume_attachment', [])
+            for attachment in attaches:
+                if attachment.get('attach_status') == 'attached':
+                    a = {'id': attachment.get('volume_id'),
+                         'attachment_id': attachment.get('id'),
+                         'volume_id': attachment.get('volume_id'),
+                         'server_id': attachment.get('instance_uuid'),
+                         'host_name': attachment.get('attached_host'),
+                         'device': attachment.get('mountpoint'),
+                         }
+                    attachments.append(a)
 
         return attachments
 
diff --git a/cinder/api/v2/volumes.py b/cinder/api/v2/volumes.py
index eda006d..bab46c1 100644
--- a/cinder/api/v2/volumes.py
+++ b/cinder/api/v2/volumes.py
@@ -43,6 +43,7 @@ SCHEDULER_HINTS_NAMESPACE =\
 
 def make_attachment(elem):
     elem.set('id')
+    elem.set('attachment_id')
     elem.set('server_id')
     elem.set('host_name')
     elem.set('volume_id')
@@ -62,6 +63,7 @@ def make_volume(elem):
     elem.set('snapshot_id')
     elem.set('source_volid')
     elem.set('consistencygroup_id')
+    elem.set('multiattach')
 
     attachments = xmlutil.SubTemplateElement(elem, 'attachments')
     attachment = xmlutil.SubTemplateElement(attachments, 'attachment',
@@ -386,6 +388,8 @@ class VolumeController(wsgi.Controller):
 
         kwargs['availability_zone'] = volume.get('availability_zone', None)
         kwargs['scheduler_hints'] = volume.get('scheduler_hints', None)
+        multiattach = volume.get('multiattach', False)
+        kwargs['multiattach'] = multiattach
 
         new_volume = self.volume_api.create(context,
                                             size,
diff --git a/cinder/backup/manager.py b/cinder/backup/manager.py
index a2f5346..060487b 100644
--- a/cinder/backup/manager.py
+++ b/cinder/backup/manager.py
@@ -41,6 +41,7 @@ from cinder.backup import rpcapi as backup_rpcapi
 from cinder import context
 from cinder import exception
 from cinder.i18n import _
+from cinder.openstack.common.gettextutils import _LE, _LI, _LW
 from cinder import manager
 from cinder.openstack.common import excutils
 from cinder.openstack.common import importutils
@@ -197,18 +198,28 @@ class BackupManager(manager.SchedulerDependentManager):
         for volume in volumes:
             volume_host = volume_utils.extract_host(volume['host'], 'backend')
             backend = self._get_volume_backend(host=volume_host)
-            if volume['status'] == 'backing-up':
-                LOG.info(_('Resetting volume %s to available '
-                           '(was backing-up).') % volume['id'])
-                mgr = self._get_manager(backend)
-                mgr.detach_volume(ctxt, volume['id'])
-            if volume['status'] == 'restoring-backup':
-                LOG.info(_('Resetting volume %s to error_restoring '
-                           '(was restoring-backup).') % volume['id'])
-                mgr = self._get_manager(backend)
-                mgr.detach_volume(ctxt, volume['id'])
-                self.db.volume_update(ctxt, volume['id'],
-                                      {'status': 'error_restoring'})
+            attachments = volume['volume_attachment']
+            if attachments:
+                if volume['status'] == 'backing-up':
+                    LOG.info(_LI('Resetting volume %s to available '
+                                 '(was backing-up).'), volume['id'])
+                    mgr = self._get_manager(backend)
+                    for attachment in attachments:
+                        if (attachment['attached_host'] == self.host and
+                           attachment['instance_uuid'] is None):
+                            mgr.detach_volume(ctxt, volume['id'],
+                                              attachment['id'])
+                if volume['status'] == 'restoring-backup':
+                    LOG.info(_LI('setting volume %s to error_restoring '
+                                 '(was restoring-backup).'), volume['id'])
+                    mgr = self._get_manager(backend)
+                    for attachment in attachments:
+                        if (attachment['attached_host'] == self.host and
+                           attachment['instance_uuid'] is None):
+                            mgr.detach_volume(ctxt, volume['id'],
+                                              attachment['id'])
+                    self.db.volume_update(ctxt, volume['id'],
+                                          {'status': 'error_restoring'})
 
         # TODO(smulcahy) implement full resume of backup and restore
         # operations on restart (rather than simply resetting)
@@ -695,4 +706,4 @@ class BackupManager(manager.SchedulerDependentManager):
             notifier_info = {'id': backup_id, 'update': {'status': status}}
             notifier = rpc.get_notifier('backupStatusUpdate')
             notifier.info(context, "backups" + '.reset_status.end',
-                          notifier_info)
\ No newline at end of file
+                          notifier_info)
diff --git a/cinder/db/api.py b/cinder/db/api.py
index 5765fb7..1088177 100644
--- a/cinder/db/api.py
+++ b/cinder/db/api.py
@@ -167,10 +167,16 @@ def volume_allocate_iscsi_target(context, volume_id, host):
     return IMPL.volume_allocate_iscsi_target(context, volume_id, host)
 
 
-def volume_attached(context, volume_id, instance_id, host_name, mountpoint):
+def volume_attach(context, values):
+    """Attach a volume."""
+    return IMPL.volume_attach(context, values)
+
+
+def volume_attached(context, volume_id, instance_id, host_name, mountpoint,
+                    attach_mode='rw'):
     """Ensure that a volume is set as attached."""
     return IMPL.volume_attached(context, volume_id, instance_id, host_name,
-                                mountpoint)
+                                mountpoint, attach_mode)
 
 
 def volume_create(context, values):
@@ -200,9 +206,9 @@ def volume_destroy(context, volume_id):
     return IMPL.volume_destroy(context, volume_id)
 
 
-def volume_detached(context, volume_id):
+def volume_detached(context, volume_id, attachment_id):
     """Ensure that a volume is set as detached."""
-    return IMPL.volume_detached(context, volume_id)
+    return IMPL.volume_detached(context, volume_id, attachment_id)
 
 
 def volume_get(context, volume_id):
@@ -248,6 +254,27 @@ def volume_update(context, volume_id, values):
     return IMPL.volume_update(context, volume_id, values)
 
 
+def volume_attachment_update(context, attachment_id, values):
+    return IMPL.volume_attachment_update(context, attachment_id, values)
+
+
+def volume_attachment_get(context, attachment_id, session=None):
+    return IMPL.volume_attachment_get(context, attachment_id, session)
+
+
+def volume_attachment_get_used_by_volume_id(context, volume_id):
+    return IMPL.volume_attachment_get_used_by_volume_id(context, volume_id)
+
+
+def volume_attachment_get_by_host(context, volume_id, host):
+    return IMPL.volume_attachment_get_by_host(context, volume_id, host)
+
+
+def volume_attachment_get_by_instance_uuid(context, volume_id, instance_uuid):
+    return IMPL.volume_attachment_get_by_instance_uuid(context, volume_id,
+                                                       instance_uuid)
+
+
 ####################
 
 
diff --git a/cinder/db/sqlalchemy/api.py b/cinder/db/sqlalchemy/api.py
index 89b763f..f7c502c 100644
--- a/cinder/db/sqlalchemy/api.py
+++ b/cinder/db/sqlalchemy/api.py
@@ -1020,19 +1020,51 @@ def volume_allocate_iscsi_target(context, volume_id, host):
     return iscsi_target_ref.target_num
 
 
+def volume_attach(context, values):
+    volume_attachment_ref = models.VolumeAttachment()
+    if not values.get('id'):
+        values['id'] = str(uuid.uuid4())
+
+    volume_attachment_ref.update(values)
+    session = get_session()
+    with session.begin():
+        volume_attachment_ref.save(session=session)
+        return volume_attachment_get(context, values['id'],
+                                     session=session)
+
+
 @require_admin_context
-def volume_attached(context, volume_id, instance_uuid, host_name, mountpoint):
+def volume_attached(context, attachment_id, instance_uuid, host_name,
+                    mountpoint, attach_mode='rw'):
+    """This method updates a volume attachment entry.
+
+    This function saves the information related to a particular
+    attachment for a volume.  It also updates the volume record
+    to mark the volume as attached.
+
+    """
     if instance_uuid and not uuidutils.is_uuid_like(instance_uuid):
         raise exception.InvalidUUID(uuid=instance_uuid)
 
     session = get_session()
     with session.begin():
-        volume_ref = _volume_get(context, volume_id, session=session)
+        volume_attachment_ref = volume_attachment_get(context, attachment_id,
+                                                      session=session)
+
+        volume_attachment_ref['mountpoint'] = mountpoint
+        volume_attachment_ref['attach_status'] = 'attached'
+        volume_attachment_ref['instance_uuid'] = instance_uuid
+        volume_attachment_ref['attached_host'] = host_name
+        volume_attachment_ref['attach_time'] = timeutils.utcnow()
+        volume_attachment_ref['attach_mode'] = attach_mode
+
+        volume_ref = _volume_get(context, volume_attachment_ref['volume_id'],
+                                 session=session)
+        volume_attachment_ref.save(session=session)
+
         volume_ref['status'] = 'in-use'
-        volume_ref['mountpoint'] = mountpoint
         volume_ref['attach_status'] = 'attached'
-        volume_ref['instance_uuid'] = instance_uuid
-        volume_ref['attached_host'] = host_name
+        volume_ref.save(session=session)
         return volume_ref
 
 
@@ -1180,18 +1212,57 @@ def volume_destroy(context, volume_id):
 
 
 @require_admin_context
-def volume_detached(context, volume_id):
+def volume_detach(context, attachment_id):
     session = get_session()
     with session.begin():
+        volume_attachment_ref = volume_attachment_get(context, attachment_id,
+                                                      session=session)
+        volume_attachment_ref['attach_status'] = 'detaching'
+        volume_attachment_ref.save(session=session)
+
+
+@require_admin_context
+def volume_detached(context, volume_id, attachment_id):
+    """This updates a volume attachment and marks it as detached.
+
+    This method also ensures that the volume entry is correctly
+    marked as either still attached/in-use or detached/available
+    if this was the last detachment made.
+
+    """
+    session = get_session()
+    with session.begin():
+        attachment = volume_attachment_get(context, attachment_id,
+                                           session=session)
+
+        # If this is already detached, attachment will be None
+        if attachment:
+            now = timeutils.utcnow()
+            attachment['attach_status'] = 'detached'
+            attachment['detach_time'] = now
+            attachment['deleted'] = True
+            attachment['deleted_at'] = now
+            attachment.save(session=session)
+
+        attachment_list = volume_attachment_get_used_by_volume_id(
+            context, volume_id, session=session)
+        remain_attachment = False
+        if attachment_list and len(attachment_list) > 0:
+            remain_attachment = True
+
         volume_ref = _volume_get(context, volume_id, session=session)
-        # Hide status update from user if we're performing a volume migration
-        if not volume_ref['migration_status']:
-            volume_ref['status'] = 'available'
-        volume_ref['mountpoint'] = None
-        volume_ref['attach_status'] = 'detached'
-        volume_ref['instance_uuid'] = None
-        volume_ref['attached_host'] = None
-        volume_ref['attach_time'] = None
+        if not remain_attachment:
+            # Hide status update from user if we're performing volume migration
+            if not volume_ref['migration_status']:
+                volume_ref['status'] = 'available'
+
+            volume_ref['attach_status'] = 'detached'
+            volume_ref.save(session=session)
+        else:
+            # Volume is still attached
+            volume_ref['status'] = 'in-use'
+            volume_ref['attach_status'] = 'attached'
+            volume_ref.save(session=session)
 
 
 @require_context
@@ -1202,12 +1273,14 @@ def _volume_get_query(context, session=None, project_only=False):
             options(joinedload('volume_metadata')).\
             options(joinedload('volume_admin_metadata')).\
             options(joinedload('volume_type')).\
+            options(joinedload('volume_attachment')).\
             options(joinedload('consistencygroup'))
     else:
         return model_query(context, models.Volume, session=session,
                            project_only=project_only).\
             options(joinedload('volume_metadata')).\
             options(joinedload('volume_type')).\
+            options(joinedload('volume_attachment')).\
             options(joinedload('consistencygroup'))
 
 
@@ -1224,6 +1297,54 @@ def _volume_get(context, volume_id, session=None):
 
 
 @require_context
+def volume_attachment_get(context, attachment_id, session=None):
+    result = model_query(context, models.VolumeAttachment,
+                         session=session).\
+        filter_by(id=attachment_id).\
+        first()
+    if not result:
+        raise exception.VolumeAttachmentNotFound(filter='attachment_id = %s' %
+                                                 attachment_id)
+    return result
+
+
+@require_context
+def volume_attachment_get_used_by_volume_id(context, volume_id, session=None):
+    result = model_query(context, models.VolumeAttachment,
+                         session=session).\
+        filter_by(volume_id=volume_id).\
+        filter(models.VolumeAttachment.attach_status != 'detached').\
+        all()
+    return result
+
+
+@require_context
+def volume_attachment_get_by_host(context, volume_id, host):
+    session = get_session()
+    with session.begin():
+        result = model_query(context, models.VolumeAttachment,
+                             session=session).\
+            filter_by(volume_id=volume_id).\
+            filter_by(attached_host=host).\
+            filter(models.VolumeAttachment.attach_status != 'detached').\
+            first()
+        return result
+
+
+@require_context
+def volume_attachment_get_by_instance_uuid(context, volume_id, instance_uuid):
+    session = get_session()
+    with session.begin():
+        result = model_query(context, models.VolumeAttachment,
+                             session=session).\
+            filter_by(volume_id=volume_id).\
+            filter_by(instance_uuid=instance_uuid).\
+            filter(models.VolumeAttachment.attach_status != 'detached').\
+            first()
+        return result
+
+
+@require_context
 def volume_get(context, volume_id):
     return _volume_get(context, volume_id)
 
@@ -1455,6 +1576,17 @@ def volume_update(context, volume_id, values):
         return volume_ref
 
 
+@require_context
+def volume_attachment_update(context, attachment_id, values):
+    session = get_session()
+    with session.begin():
+        volume_attachment_ref = volume_attachment_get(context, attachment_id,
+                                                      session=session)
+        volume_attachment_ref.update(values)
+        volume_attachment_ref.save(session=session)
+        return volume_attachment_ref
+
+
 ####################
 
 def _volume_x_metadata_get_query(context, volume_id, model, session=None):
diff --git a/cinder/db/sqlalchemy/migrate_repo/versions/028_add_volume_attachment.py b/cinder/db/sqlalchemy/migrate_repo/versions/028_add_volume_attachment.py
new file mode 100644
index 0000000..1d892f8
--- /dev/null
+++ b/cinder/db/sqlalchemy/migrate_repo/versions/028_add_volume_attachment.py
@@ -0,0 +1,147 @@
+# (c) Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+import datetime
+import uuid
+
+import six
+from sqlalchemy import Boolean, Column, DateTime
+from sqlalchemy import ForeignKey, MetaData, String, Table
+
+from cinder.i18n import _LE
+from cinder.openstack.common import log as logging
+
+LOG = logging.getLogger(__name__)
+
+CREATED_AT = datetime.datetime.now()
+
+
+def upgrade(migrate_engine):
+    """Add volume multi attachment table."""
+    meta = MetaData()
+    meta.bind = migrate_engine
+
+    # add the multiattach flag to the volumes table.
+    volumes = Table('volumes', meta, autoload=True)
+    multiattach = Column('multiattach', Boolean)
+    volumes.create_column(multiattach)
+    volumes.update().values(multiattach=False).execute()
+
+    # The new volume_attachment table
+    volume_attachment = Table(
+        'volume_attachment', meta,
+        Column('created_at', DateTime),
+        Column('updated_at', DateTime),
+        Column('deleted_at', DateTime),
+        Column('deleted', Boolean),
+        Column('id', String(length=36), primary_key=True, nullable=False),
+        Column('volume_id', String(length=36), ForeignKey('volumes.id'),
+               nullable=False),
+        Column('attached_host', String(length=255)),
+        Column('instance_uuid', String(length=36)),
+        Column('mountpoint', String(length=255)),
+        Column('attach_time', DateTime),
+        Column('detach_time', DateTime),
+        Column('attach_mode', String(length=36)),
+        Column('attach_status', String(length=255)),
+        mysql_engine='InnoDB'
+    )
+
+    try:
+        volume_attachment.create()
+    except Exception:
+        LOG.error(_LE("Table volume_attachment not created!"))
+        raise
+
+    # now migrate existing volume attachment info into the
+    # new volume_attachment table
+    volumes_list = list(volumes.select().execute())
+    for volume in volumes_list:
+        if volume.attach_status == 'attached':
+            attachment = volume_attachment.insert()
+            values = {'id': six.text_type(uuid.uuid4()),
+                      'created_at': CREATED_AT,
+                      'deleted_at': None,
+                      'deleted': 0,
+                      'volume_id': volume.id,
+                      'attached_host': volume.host,
+                      'instance_uuid': volume.instance_uuid,
+                      'mountpoint': volume.mountpoint,
+                      'attach_time': volume.attach_time,
+                      'attach_mode': 'rw',
+                      'attach_status': 'attached',
+                      }
+            attachment.execute(values)
+
+    # we have no reason to keep the columns that now
+    # exist in the volume_attachment table
+    mountpoint = volumes.columns.mountpoint
+    volumes.drop_column(mountpoint)
+    instance_uuid = volumes.columns.instance_uuid
+    volumes.drop_column(instance_uuid)
+    attach_time = volumes.columns.attach_time
+    volumes.drop_column(attach_time)
+    attached_host = volumes.columns.attached_host
+    volumes.drop_column(attached_host)
+
+
+def downgrade(migrate_engine):
+    """Remove volume_attachment table."""
+    meta = MetaData()
+    meta.bind = migrate_engine
+
+    # Put the needed volumes table columns back
+    volumes = Table('volumes', meta, autoload=True)
+    multiattach = volumes.columns.multiattach
+    volumes.drop_column(multiattach)
+
+    attached_host = Column('attached_host', String(length=255))
+    volumes.create_column(attached_host)
+    volumes.update().values(attached_host=None).execute()
+
+    attach_time = Column('attach_time', String(length=255))
+    volumes.create_column(attach_time)
+    volumes.update().values(attach_time=None).execute()
+
+    instance_uuid = Column('instance_uuid', String(length=36))
+    volumes.create_column(instance_uuid)
+    volumes.update().values(instance_uuid=None).execute()
+
+    mountpoint = Column('mountpoint', String(length=255))
+    volumes.create_column(mountpoint)
+    volumes.update().values(mountpoint=None).execute()
+
+    volume_attachment = Table('volume_attachment', meta, autoload=True)
+    attachments = list(volume_attachment.select().execute())
+    for attachment in attachments:
+        # we are going to lose data here for
+        # multiple attaches.  We'll migrate and the
+        # last update wins.
+
+        if not attachment.deleted_at:
+            volume_id = attachment.volume_id
+            volumes.update().\
+                where(volumes.c.id == volume_id).\
+                values(mountpoint=attachment.mountpoint,
+                       attached_host=attachment.attached_host,
+                       attach_time=attachment.attach_time,
+                       instance_uuid=attachment.instance_uuid).\
+                execute()
+    try:
+        volume_attachment.drop()
+
+    except Exception:
+        LOG.error(_LE("Dropping volume_attachment table failed."))
+        raise
diff --git a/cinder/db/sqlalchemy/migrate_repo/versions/028_sqlite_downgrade.sql b/cinder/db/sqlalchemy/migrate_repo/versions/028_sqlite_downgrade.sql
new file mode 100644
index 0000000..dbe89d7
--- /dev/null
+++ b/cinder/db/sqlalchemy/migrate_repo/versions/028_sqlite_downgrade.sql
@@ -0,0 +1,87 @@
+BEGIN TRANSACTION;
+
+CREATE TABLE volumes_v39 (
+    created_at DATETIME,
+    updated_at DATETIME,
+    deleted_at DATETIME,
+    deleted BOOLEAN,
+    id VARCHAR(36) NOT NULL,
+    ec2_id INTEGER,
+    user_id VARCHAR(255),
+    project_id VARCHAR(255),
+    snapshot_id VARCHAR(36),
+    host VARCHAR(255),
+    size INTEGER,
+    availability_zone VARCHAR(255),
+    status VARCHAR(255),
+    attach_status VARCHAR(255),
+    scheduled_at DATETIME,
+    launched_at DATETIME,
+    terminated_at DATETIME,
+    display_name VARCHAR(255),
+    display_description VARCHAR(255),
+    provider_location VARCHAR(255),
+    provider_auth VARCHAR(255),
+    volume_type_id VARCHAR(36),
+    source_volid VARCHAR(36),
+    bootable INTEGER,
+    provider_geometry VARCHAR(255),
+    _name_id VARCHAR(36),
+    encryption_key_id VARCHAR(36),
+    migration_status VARCHAR(255),
+    attached_host VARCHAR(255),
+    attach_time VARCHAR(255),
+    instance_uuid VARCHAR(36),
+    mountpoint VARCHAR(255),
+    consistencygroup_id VARCHAR(36),
+    replication_status VARCHAR(255),
+    replication_extended_status VARCHAR(255),
+    replication_driver_data VARCHAR(255),
+    PRIMARY KEY (id)
+);
+
+INSERT INTO volumes_v39
+    SELECT volumes.created_at,
+        volumes.updated_at,
+        volumes.deleted_at,
+        volumes.deleted,
+        volumes.id,
+        volumes.ec2_id,
+        volumes.user_id,
+        volumes.project_id,
+        volumes.snapshot_id,
+        volumes.host,
+        volumes.size,
+        volumes.availability_zone,
+        volumes.status,
+        volumes.attach_status,
+        volumes.scheduled_at,
+        volumes.launched_at,
+        volumes.terminated_at,
+        volumes.display_name,
+        volumes.display_description,
+        volumes.provider_location,
+        volumes.provider_auth,
+        volumes.volume_type_id,
+        volumes.source_volid,
+        volumes.bootable,
+        volumes.provider_geometry,
+        volumes._name_id,
+        volumes.encryption_key_id,
+        volumes.migration_status,
+        volume_attachment.attached_host,
+        volume_attachment.attach_time,
+        volume_attachment.instance_uuid,
+        volume_attachment.mountpoint,
+        volumes.consistencygroup_id,
+        volumes.replication_status,
+        volumes.replication_extended_status,
+        volumes.replication_driver_data
+    FROM volumes
+    LEFT OUTER JOIN volume_attachment
+    ON volumes.id=volume_attachment.volume_id;
+
+DROP TABLE volumes;
+ALTER TABLE volumes_v39 RENAME TO volumes;
+DROP TABLE volume_attachment;
+COMMIT;
diff --git a/cinder/db/sqlalchemy/models.py b/cinder/db/sqlalchemy/models.py
index 466ef94..ab3985d 100644
--- a/cinder/db/sqlalchemy/models.py
+++ b/cinder/db/sqlalchemy/models.py
@@ -129,10 +129,6 @@ class Volume(BASE, CinderBase):
     host = Column(String(255))  # , ForeignKey('hosts.id'))
     size = Column(Integer)
     availability_zone = Column(String(255))  # TODO(vish): foreign key?
-    instance_uuid = Column(String(36))
-    attached_host = Column(String(255))
-    mountpoint = Column(String(255))
-    attach_time = Column(String(255))  # TODO(vish): datetime
     status = Column(String(255))  # TODO(vish): enum?
     attach_status = Column(String(255))  # TODO(vish): enum
     migration_status = Column(String(255))
@@ -156,6 +152,7 @@ class Volume(BASE, CinderBase):
 
     deleted = Column(Boolean, default=False)
     bootable = Column(Boolean, default=False)
+    multiattach = Column(Boolean, default=False)
 
     replication_status = Column(String(255))
     replication_extended_status = Column(String(255))
@@ -196,6 +193,26 @@ class VolumeAdminMetadata(BASE, CinderBase):
                           'VolumeAdminMetadata.deleted == False)')
 
 
+class VolumeAttachment(BASE, CinderBase):
+    """Represents a volume attachment for a vm."""
+    __tablename__ = 'volume_attachment'
+    id = Column(String(36), primary_key=True)
+
+    volume_id = Column(String(36), ForeignKey('volumes.id'), nullable=False)
+    volume = relationship(Volume, backref="volume_attachment",
+                          foreign_keys=volume_id,
+                          primaryjoin='and_('
+                          'VolumeAttachment.volume_id == Volume.id,'
+                          'VolumeAttachment.deleted == False)')
+    instance_uuid = Column(String(36))
+    attached_host = Column(String(255))
+    mountpoint = Column(String(255))
+    attach_time = Column(DateTime)
+    detach_time = Column(DateTime)
+    attach_status = Column(String(255))
+    attach_mode = Column(String(255))
+
+
 class VolumeTypes(BASE, CinderBase):
     """Represent possible volume_types of volumes offered."""
     __tablename__ = "volume_types"
@@ -559,6 +576,7 @@ def register_models():
               Volume,
               VolumeMetadata,
               VolumeAdminMetadata,
+              VolumeAttachment,
               SnapshotMetadata,
               Transfer,
               VolumeTypeExtraSpecs,
diff --git a/cinder/exception.py b/cinder/exception.py
index fc747ce..aa87070 100644
--- a/cinder/exception.py
+++ b/cinder/exception.py
@@ -235,6 +235,11 @@ class VolumeNotFound(NotFound):
     message = _("Volume %(volume_id)s could not be found.")
 
 
+class VolumeAttachmentNotFound(NotFound):
+    message = _("Volume attachment could not be found with "
+                "filter: %(filter)s .")
+
+
 class VolumeMetadataNotFound(NotFound):
     message = _("Volume %(volume_id)s has no metadata with "
                 "key %(metadata_key)s.")
diff --git a/cinder/tests/api/contrib/test_admin_actions.py b/cinder/tests/api/contrib/test_admin_actions.py
index 64963f9..6fae100 100644
--- a/cinder/tests/api/contrib/test_admin_actions.py
+++ b/cinder/tests/api/contrib/test_admin_actions.py
@@ -368,15 +368,14 @@ class AdminActionsTest(test.TestCase):
         svc = self.start_service('volume', host='test')
         self.volume_api.reserve_volume(ctx, volume)
         mountpoint = '/dev/vbd'
-        self.volume_api.attach(ctx, volume, stubs.FAKE_UUID, None,
-                               mountpoint, 'rw')
+        attachment = self.volume_api.attach(ctx, volume, stubs.FAKE_UUID,
+                                            None, mountpoint, 'rw')
         # volume is attached
         volume = db.volume_get(ctx, volume['id'])
         self.assertEqual(volume['status'], 'in-use')
-        self.assertEqual(volume['instance_uuid'], stubs.FAKE_UUID)
-        self.assertIsNone(volume['attached_host'])
-        self.assertEqual(volume['mountpoint'], mountpoint)
-        self.assertEqual(volume['attach_status'], 'attached')
+        self.assertEqual(attachment['instance_uuid'], stubs.FAKE_UUID)
+        self.assertEqual(attachment['mountpoint'], mountpoint)
+        self.assertEqual(attachment['attach_status'], 'attached')
         admin_metadata = volume['volume_admin_metadata']
         self.assertEqual(len(admin_metadata), 2)
         self.assertEqual(admin_metadata[0]['key'], 'readonly')
@@ -391,7 +390,8 @@ class AdminActionsTest(test.TestCase):
         req.method = 'POST'
         req.headers['content-type'] = 'application/json'
         # request status of 'error'
-        req.body = jsonutils.dumps({'os-force_detach': None})
+        req.body = jsonutils.dumps({'os-force_detach':
+                                    {'attachment_id': attachment['id']}})
         # attach admin context to request
         req.environ['cinder.context'] = ctx
         # make request
@@ -399,12 +399,12 @@ class AdminActionsTest(test.TestCase):
         # request is accepted
         self.assertEqual(resp.status_int, 202)
         volume = db.volume_get(ctx, volume['id'])
+        self.assertRaises(exception.VolumeAttachmentNotFound,
+                          db.volume_attachment_get,
+                          ctx, attachment['id'])
+
         # status changed to 'available'
         self.assertEqual(volume['status'], 'available')
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertIsNone(volume['attached_host'])
-        self.assertIsNone(volume['mountpoint'])
-        self.assertEqual(volume['attach_status'], 'detached')
         admin_metadata = volume['volume_admin_metadata']
         self.assertEqual(len(admin_metadata), 1)
         self.assertEqual(admin_metadata[0]['key'], 'readonly')
@@ -421,17 +421,18 @@ class AdminActionsTest(test.TestCase):
         connector = {'initiator': 'iqn.2012-07.org.fake:01'}
         # start service to handle rpc messages for attach requests
         svc = self.start_service('volume', host='test')
-        self.volume_api.reserve_volume(ctx, volume)
+        self.volume_api.initialize_connection(ctx, volume, connector)
         mountpoint = '/dev/vbd'
         host_name = 'fake-host'
-        self.volume_api.attach(ctx, volume, None, host_name, mountpoint, 'ro')
+        attachment = self.volume_api.attach(ctx, volume, None, host_name,
+                                            mountpoint, 'ro')
         # volume is attached
         volume = db.volume_get(ctx, volume['id'])
         self.assertEqual(volume['status'], 'in-use')
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertEqual(volume['attached_host'], host_name)
-        self.assertEqual(volume['mountpoint'], mountpoint)
-        self.assertEqual(volume['attach_status'], 'attached')
+        self.assertIsNone(attachment['instance_uuid'])
+        self.assertEqual(attachment['attached_host'], host_name)
+        self.assertEqual(attachment['mountpoint'], mountpoint)
+        self.assertEqual(attachment['attach_status'], 'attached')
         admin_metadata = volume['volume_admin_metadata']
         self.assertEqual(len(admin_metadata), 2)
         self.assertEqual(admin_metadata[0]['key'], 'readonly')
@@ -446,7 +447,8 @@ class AdminActionsTest(test.TestCase):
         req.method = 'POST'
         req.headers['content-type'] = 'application/json'
         # request status of 'error'
-        req.body = jsonutils.dumps({'os-force_detach': None})
+        req.body = jsonutils.dumps({'os-force_detach':
+                                    {'attachment_id': attachment['id']}})
         # attach admin context to request
         req.environ['cinder.context'] = ctx
         # make request
@@ -454,12 +456,11 @@ class AdminActionsTest(test.TestCase):
         # request is accepted
         self.assertEqual(resp.status_int, 202)
         volume = db.volume_get(ctx, volume['id'])
+        self.assertRaises(exception.VolumeAttachmentNotFound,
+                          db.volume_attachment_get,
+                          ctx, attachment['id'])
         # status changed to 'available'
         self.assertEqual(volume['status'], 'available')
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertIsNone(volume['attached_host'])
-        self.assertIsNone(volume['mountpoint'])
-        self.assertEqual(volume['attach_status'], 'detached')
         admin_metadata = volume['volume_admin_metadata']
         self.assertEqual(len(admin_metadata), 1)
         self.assertEqual(admin_metadata[0]['key'], 'readonly')
@@ -478,11 +479,10 @@ class AdminActionsTest(test.TestCase):
         # start service to handle rpc messages for attach requests
         svc = self.start_service('volume', host='test')
         self.volume_api.reserve_volume(ctx, volume)
-        mountpoint = '/dev/vbd'
-        self.volume_api.attach(ctx, volume, stubs.FAKE_UUID, None,
-                               mountpoint, 'rw')
         conn_info = self.volume_api.initialize_connection(ctx,
                                                           volume, connector)
+        self.volume_api.attach(ctx, volume, fakes.get_fake_uuid(), None,
+                               '/dev/vbd0', 'rw')
         self.assertEqual(conn_info['data']['access_mode'], 'rw')
         self.assertRaises(exception.InvalidVolume,
                           self.volume_api.attach,
@@ -490,15 +490,7 @@ class AdminActionsTest(test.TestCase):
                           volume,
                           fakes.get_fake_uuid(),
                           None,
-                          mountpoint,
-                          'rw')
-        self.assertRaises(exception.InvalidVolume,
-                          self.volume_api.attach,
-                          ctx,
-                          volume,
-                          fakes.get_fake_uuid(),
-                          None,
-                          mountpoint,
+                          '/dev/vdb1',
                           'ro')
         # cleanup
         svc.stop()
@@ -514,9 +506,9 @@ class AdminActionsTest(test.TestCase):
         # start service to handle rpc messages for attach requests
         svc = self.start_service('volume', host='test')
         self.volume_api.reserve_volume(ctx, volume)
-        mountpoint = '/dev/vbd'
-        host_name = 'fake_host'
-        self.volume_api.attach(ctx, volume, None, host_name, mountpoint, 'rw')
+        self.volume_api.initialize_connection(ctx, volume, connector)
+        self.volume_api.attach(ctx, volume, None, 'fake_host1',
+                               '/dev/vbd0', 'rw')
         conn_info = self.volume_api.initialize_connection(ctx,
                                                           volume, connector)
         conn_info['data']['access_mode'] = 'rw'
@@ -525,16 +517,8 @@ class AdminActionsTest(test.TestCase):
                           ctx,
                           volume,
                           None,
-                          host_name,
-                          mountpoint,
-                          'rw')
-        self.assertRaises(exception.InvalidVolume,
-                          self.volume_api.attach,
-                          ctx,
-                          volume,
-                          None,
-                          host_name,
-                          mountpoint,
+                          'fake_host2',
+                          '/dev/vbd1',
                           'ro')
         # cleanup
         svc.stop()
@@ -563,19 +547,23 @@ class AdminActionsTest(test.TestCase):
                                         'provider_location': '', 'size': 1})
         # start service to handle rpc messages for attach requests
         svc = self.start_service('volume', host='test')
-        values = {'status': 'attaching',
-                  'instance_uuid': fakes.get_fake_uuid()}
-        db.volume_update(ctx, volume['id'], values)
+        self.volume_api.reserve_volume(ctx, volume)
+        values = {'volume_id': volume['id'],
+                  'attach_status': 'attaching',
+                  'attach_time': timeutils.utcnow(),
+                  'instance_uuid': 'abc123',
+                  }
+        db.volume_attach(ctx, values)
+        db.volume_admin_metadata_update(ctx, volume['id'],
+                                        {"attached_mode": 'rw'}, False)
         mountpoint = '/dev/vbd'
-        self.assertRaises(exception.InvalidVolume,
-                          self.volume_api.attach,
-                          ctx,
-                          volume,
-                          stubs.FAKE_UUID,
-                          None,
-                          mountpoint,
-                          'rw')
-        # cleanup
+        attachment = self.volume_api.attach(ctx, volume,
+                                            stubs.FAKE_UUID, None,
+                                            mountpoint, 'rw')
+
+        self.assertEqual(stubs.FAKE_UUID, attachment['instance_uuid'])
+        self.assertEqual(volume['id'], attachment['volume_id'], volume['id'])
+        self.assertEqual('attached', attachment['attach_status'])
         svc.stop()
 
     def test_attach_attaching_volume_with_different_mode(self):
diff --git a/cinder/tests/api/contrib/test_volume_actions.py b/cinder/tests/api/contrib/test_volume_actions.py
index c8c1feb..87cf572 100644
--- a/cinder/tests/api/contrib/test_volume_actions.py
+++ b/cinder/tests/api/contrib/test_volume_actions.py
@@ -37,7 +37,7 @@ CONF = cfg.CONF
 
 class VolumeActionsTest(test.TestCase):
 
-    _actions = ('os-detach', 'os-reserve', 'os-unreserve')
+    _actions = ('os-reserve', 'os-unreserve')
 
     _methods = ('attach', 'detach', 'reserve_volume', 'unreserve_volume')
 
@@ -179,6 +179,16 @@ class VolumeActionsTest(test.TestCase):
         res = req.get_response(fakes.wsgi_app())
         self.assertEqual(res.status_int, 202)
 
+    def test_detach(self):
+        body = {'os-detach': {'attachment_id': 'fakeuuid'}}
+        req = webob.Request.blank('/v2/fake/volumes/1/action')
+        req.method = "POST"
+        req.body = jsonutils.dumps(body)
+        req.headers["content-type"] = "application/json"
+
+        res = req.get_response(fakes.wsgi_app())
+        self.assertEqual(202, res.status_int)
+
     def test_attach_with_invalid_arguments(self):
         # Invalid request to attach volume an invalid target
         body = {'os-attach': {'mountpoint': '/dev/vdc'}}
diff --git a/cinder/tests/api/v1/stubs.py b/cinder/tests/api/v1/stubs.py
index 8f68830..764bfee 100644
--- a/cinder/tests/api/v1/stubs.py
+++ b/cinder/tests/api/v1/stubs.py
@@ -29,9 +29,6 @@ def stub_volume(id, **kwargs):
         'host': 'fakehost',
         'size': 1,
         'availability_zone': 'fakeaz',
-        'instance_uuid': 'fakeuuid',
-        'attached_host': None,
-        'mountpoint': '/',
         'attached_mode': 'rw',
         'status': 'fakestatus',
         'migration_status': None,
@@ -46,6 +43,8 @@ def stub_volume(id, **kwargs):
         'volume_type_id': '3e196c20-3c06-11e2-81c1-0800200c9a66',
         'volume_metadata': [],
         'volume_type': {'name': 'vol_type_name'},
+        'volume_attachment': [],
+        'multiattach': False,
         'readonly': 'False'}
 
     volume.update(kwargs)
diff --git a/cinder/tests/api/v1/test_volumes.py b/cinder/tests/api/v1/test_volumes.py
index 9eb8b01..5365818 100644
--- a/cinder/tests/api/v1/test_volumes.py
+++ b/cinder/tests/api/v1/test_volumes.py
@@ -85,11 +85,8 @@ class VolumeApiTest(test.TestCase):
                                'availability_zone': 'zone1:host1',
                                'display_name': 'Volume Test Name',
                                'encrypted': False,
-                               'attachments': [{'device': '/',
-                                                'server_id': 'fakeuuid',
-                                                'host_name': None,
-                                                'id': '1',
-                                                'volume_id': '1'}],
+                               'attachments': [],
+                               'multiattach': 'false',
                                'bootable': 'false',
                                'volume_type': 'vol_type_name',
                                'snapshot_id': None,
@@ -176,11 +173,8 @@ class VolumeApiTest(test.TestCase):
                                'availability_zone': 'nova',
                                'display_name': 'Volume Test Name',
                                'encrypted': False,
-                               'attachments': [{'device': '/',
-                                                'server_id': 'fakeuuid',
-                                                'host_name': None,
-                                                'id': '1',
-                                                'volume_id': '1'}],
+                               'attachments': [],
+                               'multiattach': 'false',
                                'bootable': 'false',
                                'volume_type': 'vol_type_name',
                                'image_id': test_id,
@@ -258,13 +252,8 @@ class VolumeApiTest(test.TestCase):
             'availability_zone': 'fakeaz',
             'display_name': 'Updated Test Name',
             'encrypted': False,
-            'attachments': [{
-                'id': '1',
-                'volume_id': '1',
-                'server_id': 'fakeuuid',
-                'host_name': None,
-                'device': '/'
-            }],
+            'attachments': [],
+            'multiattach': 'false',
             'bootable': 'false',
             'volume_type': 'vol_type_name',
             'snapshot_id': None,
@@ -294,13 +283,8 @@ class VolumeApiTest(test.TestCase):
             'availability_zone': 'fakeaz',
             'display_name': 'displayname',
             'encrypted': False,
-            'attachments': [{
-                'id': '1',
-                'volume_id': '1',
-                'server_id': 'fakeuuid',
-                'host_name': None,
-                'device': '/'
-            }],
+            'attachments': [],
+            'multiattach': 'false',
             'bootable': 'false',
             'volume_type': 'vol_type_name',
             'snapshot_id': None,
@@ -328,6 +312,10 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         updates = {
             "display_name": "Updated Test Name",
@@ -339,18 +327,20 @@ class VolumeApiTest(test.TestCase):
         req.environ['cinder.context'] = admin_ctx
         res_dict = self.controller.update(req, '1', body)
         expected = {'volume': {
-            'status': 'fakestatus',
+            'status': 'in-use',
             'display_description': 'displaydesc',
             'availability_zone': 'fakeaz',
             'display_name': 'Updated Test Name',
             'encrypted': False,
             'attachments': [{
+                'attachment_id': attachment['id'],
                 'id': '1',
                 'volume_id': '1',
-                'server_id': 'fakeuuid',
+                'server_id': stubs.FAKE_UUID,
                 'host_name': None,
                 'device': '/'
             }],
+            'multiattach': 'false',
             'bootable': 'false',
             'volume_type': 'None',
             'snapshot_id': None,
@@ -400,11 +390,8 @@ class VolumeApiTest(test.TestCase):
                                  'availability_zone': 'fakeaz',
                                  'display_name': 'displayname',
                                  'encrypted': False,
-                                 'attachments': [{'device': '/',
-                                                  'server_id': 'fakeuuid',
-                                                  'host_name': None,
-                                                  'id': '1',
-                                                  'volume_id': '1'}],
+                                 'attachments': [],
+                                 'multiattach': 'false',
                                  'bootable': 'false',
                                  'volume_type': 'vol_type_name',
                                  'snapshot_id': None,
@@ -430,21 +417,28 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         req = fakes.HTTPRequest.blank('/v1/volumes')
         admin_ctx = context.RequestContext('admin', 'fakeproject', True)
         req.environ['cinder.context'] = admin_ctx
         res_dict = self.controller.index(req)
-        expected = {'volumes': [{'status': 'fakestatus',
+        expected = {'volumes': [{'status': 'in-use',
                                  'display_description': 'displaydesc',
                                  'availability_zone': 'fakeaz',
                                  'display_name': 'displayname',
                                  'encrypted': False,
-                                 'attachments': [{'device': '/',
-                                                  'server_id': 'fakeuuid',
-                                                  'host_name': None,
-                                                  'id': '1',
-                                                  'volume_id': '1'}],
+                                 'attachments': [
+                                     {'attachment_id': attachment['id'],
+                                      'device': '/',
+                                      'server_id': stubs.FAKE_UUID,
+                                      'host_name': None,
+                                      'id': '1',
+                                      'volume_id': '1'}],
+                                 'multiattach': 'false',
                                  'bootable': 'false',
                                  'volume_type': 'None',
                                  'snapshot_id': None,
@@ -469,11 +463,8 @@ class VolumeApiTest(test.TestCase):
                                  'availability_zone': 'fakeaz',
                                  'display_name': 'displayname',
                                  'encrypted': False,
-                                 'attachments': [{'device': '/',
-                                                  'server_id': 'fakeuuid',
-                                                  'host_name': None,
-                                                  'id': '1',
-                                                  'volume_id': '1'}],
+                                 'attachments': [],
+                                 'multiattach': 'false',
                                  'bootable': 'false',
                                  'volume_type': 'vol_type_name',
                                  'snapshot_id': None,
@@ -499,21 +490,28 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         req = fakes.HTTPRequest.blank('/v1/volumes/detail')
         admin_ctx = context.RequestContext('admin', 'fakeproject', True)
         req.environ['cinder.context'] = admin_ctx
         res_dict = self.controller.index(req)
-        expected = {'volumes': [{'status': 'fakestatus',
+        expected = {'volumes': [{'status': 'in-use',
                                  'display_description': 'displaydesc',
                                  'availability_zone': 'fakeaz',
                                  'display_name': 'displayname',
                                  'encrypted': False,
-                                 'attachments': [{'device': '/',
-                                                  'server_id': 'fakeuuid',
-                                                  'host_name': None,
-                                                  'id': '1',
-                                                  'volume_id': '1'}],
+                                 'attachments': [
+                                     {'attachment_id': attachment['id'],
+                                      'device': '/',
+                                      'server_id': stubs.FAKE_UUID,
+                                      'host_name': None,
+                                      'id': '1',
+                                      'volume_id': '1'}],
+                                 'multiattach': 'false',
                                  'bootable': 'false',
                                  'volume_type': 'None',
                                  'snapshot_id': None,
@@ -536,11 +534,8 @@ class VolumeApiTest(test.TestCase):
                                'availability_zone': 'fakeaz',
                                'display_name': 'displayname',
                                'encrypted': False,
-                               'attachments': [{'device': '/',
-                                                'server_id': 'fakeuuid',
-                                                'host_name': None,
-                                                'id': '1',
-                                                'volume_id': '1'}],
+                               'attachments': [],
+                               'multiattach': 'false',
                                'bootable': 'false',
                                'volume_type': 'vol_type_name',
                                'snapshot_id': None,
@@ -569,6 +564,7 @@ class VolumeApiTest(test.TestCase):
                                'display_name': 'displayname',
                                'encrypted': False,
                                'attachments': [],
+                               'multiattach': 'false',
                                'bootable': 'false',
                                'volume_type': 'vol_type_name',
                                'snapshot_id': None,
@@ -594,11 +590,8 @@ class VolumeApiTest(test.TestCase):
                                'availability_zone': 'fakeaz',
                                'display_name': 'displayname',
                                'encrypted': False,
-                               'attachments': [{'device': '/',
-                                                'server_id': 'fakeuuid',
-                                                'host_name': None,
-                                                'id': '1',
-                                                'volume_id': '1'}],
+                               'attachments': [],
+                               'multiattach': 'false',
                                'bootable': 'true',
                                'volume_type': 'vol_type_name',
                                'snapshot_id': None,
@@ -661,21 +654,28 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         req = fakes.HTTPRequest.blank('/v1/volumes/1')
         admin_ctx = context.RequestContext('admin', 'fakeproject', True)
         req.environ['cinder.context'] = admin_ctx
         res_dict = self.controller.show(req, '1')
-        expected = {'volume': {'status': 'fakestatus',
+        expected = {'volume': {'status': 'in-use',
                                'display_description': 'displaydesc',
                                'availability_zone': 'fakeaz',
                                'display_name': 'displayname',
                                'encrypted': False,
-                               'attachments': [{'device': '/',
-                                                'server_id': 'fakeuuid',
-                                                'host_name': None,
-                                                'id': '1',
-                                                'volume_id': '1'}],
+                               'attachments': [
+                                   {'attachment_id': attachment['id'],
+                                    'device': '/',
+                                    'server_id': stubs.FAKE_UUID,
+                                    'host_name': None,
+                                    'id': '1',
+                                    'volume_id': '1'}],
+                               'multiattach': 'false',
                                'bootable': 'false',
                                'volume_type': 'None',
                                'snapshot_id': None,
diff --git a/cinder/tests/api/v2/stubs.py b/cinder/tests/api/v2/stubs.py
index bb4a841..9ec2d19 100644
--- a/cinder/tests/api/v2/stubs.py
+++ b/cinder/tests/api/v2/stubs.py
@@ -30,9 +30,6 @@ def stub_volume(id, **kwargs):
         'host': 'fakehost',
         'size': 1,
         'availability_zone': 'fakeaz',
-        'instance_uuid': 'fakeuuid',
-        'attached_host': None,
-        'mountpoint': '/',
         'status': 'fakestatus',
         'migration_status': None,
         'attach_status': 'attached',
@@ -53,7 +50,10 @@ def stub_volume(id, **kwargs):
         'volume_type': {'name': 'vol_type_name'},
         'replication_status': 'disabled',
         'replication_extended_status': None,
-        'replication_driver_data': None}
+        'replication_driver_data': None,
+        'volume_attachment': [],
+        'multiattach': False,
+    }
 
     volume.update(kwargs)
     if kwargs.get('volume_glance_metadata', None):
diff --git a/cinder/tests/api/v2/test_volumes.py b/cinder/tests/api/v2/test_volumes.py
index 1fded89..1418f35 100644
--- a/cinder/tests/api/v2/test_volumes.py
+++ b/cinder/tests/api/v2/test_volumes.py
@@ -86,12 +86,7 @@ class VolumeApiTest(test.TestCase):
         body = {"volume": vol}
         req = fakes.HTTPRequest.blank('/v2/volumes')
         res_dict = self.controller.create(req, body)
-        ex = {'volume': {'attachments':
-                         [{'device': '/',
-                           'host_name': None,
-                           'id': '1',
-                           'server_id': 'fakeuuid',
-                           'volume_id': '1'}],
+        ex = {'volume': {'attachments': [],
                          'availability_zone': 'zone1:host1',
                          'bootable': 'false',
                          'consistencygroup_id': None,
@@ -106,6 +101,7 @@ class VolumeApiTest(test.TestCase):
                          'metadata': {},
                          'name': 'Volume Test Name',
                          'replication_status': 'disabled',
+                         'multiattach': False,
                          'size': 100,
                          'snapshot_id': None,
                          'source_volid': None,
@@ -211,6 +207,93 @@ class VolumeApiTest(test.TestCase):
                           self.controller.create,
                           req, body)
 
+    def test_volume_create_with_image_ref(self):
+        self.stubs.Set(volume_api.API, 'get', stubs.stub_volume_get)
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {"size": '1',
+               "name": "Volume Test Name",
+               "description": "Volume Test Desc",
+               "availability_zone": "nova",
+               "imageRef": 'c905cedb-7281-47e4-8a62-f26bc5fc4c77'}
+        ex = {'volume': {'attachments': [],
+                         'availability_zone': 'nova',
+                         'bootable': 'false',
+                         'consistencygroup_id': None,
+                         'created_at': datetime.datetime(1, 1, 1, 1, 1, 1),
+                         'description': 'Volume Test Desc',
+                         'encrypted': False,
+                         'id': '1',
+                         'links':
+                         [{'href': 'http://localhost/v2/fakeproject/volumes/1',
+                           'rel': 'self'},
+                          {'href': 'http://localhost/fakeproject/volumes/1',
+                           'rel': 'bookmark'}],
+                         'metadata': {},
+                         'name': 'Volume Test Name',
+                         'replication_status': 'disabled',
+                         'multiattach': False,
+                         'size': '1',
+                         'snapshot_id': None,
+                         'source_volid': None,
+                         'status': 'fakestatus',
+                         'user_id': 'fakeuser',
+                         'volume_type': 'vol_type_name'}}
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        res_dict = self.controller.create(req, body)
+        self.assertEqual(ex, res_dict)
+
+    def test_volume_create_with_image_ref_is_integer(self):
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {
+            "size": '1',
+            "name": "Volume Test Name",
+            "description": "Volume Test Desc",
+            "availability_zone": "cinder",
+            "imageRef": 1234,
+        }
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
+    def test_volume_create_with_image_ref_not_uuid_format(self):
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {
+            "size": '1',
+            "name": "Volume Test Name",
+            "description": "Volume Test Desc",
+            "availability_zone": "cinder",
+            "imageRef": '12345'
+        }
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
+    def test_volume_create_with_image_ref_with_empty_string(self):
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {"size": 1,
+               "display_name": "Volume Test Name",
+               "display_description": "Volume Test Desc",
+               "availability_zone": "cinder",
+               "imageRef": ''}
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
     def test_volume_create_with_image_id(self):
         self.stubs.Set(volume_api.API, 'get', stubs.stub_volume_get)
         self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
@@ -221,11 +304,7 @@ class VolumeApiTest(test.TestCase):
                "description": "Volume Test Desc",
                "availability_zone": "nova",
                "imageRef": 'c905cedb-7281-47e4-8a62-f26bc5fc4c77'}
-        ex = {'volume': {'attachments': [{'device': '/',
-                                          'host_name': None,
-                                          'id': '1',
-                                          'server_id': 'fakeuuid',
-                                          'volume_id': '1'}],
+        ex = {'volume': {'attachments': [],
                          'availability_zone': 'nova',
                          'bootable': 'false',
                          'consistencygroup_id': None,
@@ -241,6 +320,7 @@ class VolumeApiTest(test.TestCase):
                          'metadata': {},
                          'name': 'Volume Test Name',
                          'replication_status': 'disabled',
+                         'multiattach': False,
                          'size': '1',
                          'snapshot_id': None,
                          'source_volid': None,
@@ -301,6 +381,83 @@ class VolumeApiTest(test.TestCase):
                           req,
                           body)
 
+    def test_volume_create_with_image_name(self):
+        self.stubs.Set(db, 'volume_get', stubs.stub_volume_get_db)
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+
+        test_id = "Fedora-x86_64-20-20140618-sda"
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {"size": '1',
+               "name": "Volume Test Name",
+               "description": "Volume Test Desc",
+               "availability_zone": "nova",
+               "imageRef": test_id}
+        ex = {'volume': {'attachments': [],
+                         'availability_zone': 'nova',
+                         'bootable': 'false',
+                         'consistencygroup_id': None,
+                         'created_at': datetime.datetime(1900, 1, 1, 1, 1, 1),
+                         'description': 'Volume Test Desc',
+                         'encrypted': False,
+                         'id': '1',
+                         'links':
+                         [{'href': 'http://localhost/v2/fakeproject/volumes/1',
+                           'rel': 'self'},
+                          {'href': 'http://localhost/fakeproject/volumes/1',
+                           'rel': 'bookmark'}],
+                         'metadata': {},
+                         'name': 'Volume Test Name',
+                         'replication_status': 'disabled',
+                         'multiattach': False,
+                         'size': '1',
+                         'snapshot_id': None,
+                         'source_volid': None,
+                         'status': 'fakestatus',
+                         'user_id': 'fakeuser',
+                         'volume_type': 'vol_type_name'}}
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
+    def test_volume_create_with_image_name_has_multiple(self):
+        self.stubs.Set(db, 'volume_get', stubs.stub_volume_get_db)
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+
+        test_id = "multi"
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {"size": '1',
+               "name": "Volume Test Name",
+               "description": "Volume Test Desc",
+               "availability_zone": "nova",
+               "imageRef": test_id}
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
+    def test_volume_create_with_image_name_no_match(self):
+        self.stubs.Set(db, 'volume_get', stubs.stub_volume_get_db)
+        self.stubs.Set(volume_api.API, "create", stubs.stub_volume_create)
+
+        test_id = "MissingName"
+        self.ext_mgr.extensions = {'os-image-create': 'fake'}
+        vol = {"size": '1',
+               "name": "Volume Test Name",
+               "description": "Volume Test Desc",
+               "availability_zone": "nova",
+               "imageRef": test_id}
+        body = {"volume": vol}
+        req = fakes.HTTPRequest.blank('/v2/volumes')
+        self.assertRaises(webob.exc.HTTPBadRequest,
+                          self.controller.create,
+                          req,
+                          body)
+
     def test_volume_update(self):
         self.stubs.Set(volume_api.API, 'get', stubs.stub_volume_get)
         self.stubs.Set(volume_api.API, "update", stubs.stub_volume_update)
@@ -322,15 +479,8 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'Updated Test Name',
                 'replication_status': 'disabled',
-                'attachments': [
-                    {
-                        'id': '1',
-                        'volume_id': '1',
-                        'server_id': 'fakeuuid',
-                        'host_name': None,
-                        'device': '/',
-                    }
-                ],
+                'multiattach': False,
+                'attachments': [],
                 'user_id': 'fakeuser',
                 'volume_type': 'vol_type_name',
                 'snapshot_id': None,
@@ -376,15 +526,8 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'Updated Test Name',
                 'replication_status': 'disabled',
-                'attachments': [
-                    {
-                        'id': '1',
-                        'volume_id': '1',
-                        'server_id': 'fakeuuid',
-                        'host_name': None,
-                        'device': '/',
-                    }
-                ],
+                'multiattach': False,
+                'attachments': [],
                 'user_id': 'fakeuser',
                 'volume_type': 'vol_type_name',
                 'snapshot_id': None,
@@ -433,15 +576,8 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'New Name',
                 'replication_status': 'disabled',
-                'attachments': [
-                    {
-                        'id': '1',
-                        'volume_id': '1',
-                        'server_id': 'fakeuuid',
-                        'host_name': None,
-                        'device': '/',
-                    }
-                ],
+                'multiattach': False,
+                'attachments': [],
                 'user_id': 'fakeuser',
                 'volume_type': 'vol_type_name',
                 'snapshot_id': None,
@@ -485,13 +621,8 @@ class VolumeApiTest(test.TestCase):
             'consistencygroup_id': None,
             'name': 'displayname',
             'replication_status': 'disabled',
-            'attachments': [{
-                'id': '1',
-                'volume_id': '1',
-                'server_id': 'fakeuuid',
-                'host_name': None,
-                'device': '/',
-            }],
+            'multiattach': False,
+            'attachments': [],
             'user_id': 'fakeuser',
             'volume_type': 'vol_type_name',
             'snapshot_id': None,
@@ -529,6 +660,10 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         updates = {
             "name": "Updated Test Name",
@@ -540,7 +675,7 @@ class VolumeApiTest(test.TestCase):
         req.environ['cinder.context'] = admin_ctx
         res_dict = self.controller.update(req, '1', body)
         expected = {'volume': {
-            'status': 'fakestatus',
+            'status': 'in-use',
             'description': 'displaydesc',
             'encrypted': False,
             'availability_zone': 'fakeaz',
@@ -548,10 +683,12 @@ class VolumeApiTest(test.TestCase):
             'consistencygroup_id': None,
             'name': 'Updated Test Name',
             'replication_status': 'disabled',
+            'multiattach': False,
             'attachments': [{
                 'id': '1',
+                'attachment_id': attachment['id'],
                 'volume_id': '1',
-                'server_id': 'fakeuuid',
+                'server_id': stubs.FAKE_UUID,
                 'host_name': None,
                 'device': '/',
             }],
@@ -575,8 +712,8 @@ class VolumeApiTest(test.TestCase):
                 }
             ],
         }}
-        self.assertEqual(res_dict, expected)
-        self.assertEqual(len(fake_notifier.NOTIFICATIONS), 2)
+        self.assertEqual(expected, res_dict)
+        self.assertEqual(2, len(fake_notifier.NOTIFICATIONS))
 
     def test_update_empty_body(self):
         body = {}
@@ -653,15 +790,8 @@ class VolumeApiTest(test.TestCase):
                     'consistencygroup_id': None,
                     'name': 'displayname',
                     'replication_status': 'disabled',
-                    'attachments': [
-                        {
-                            'device': '/',
-                            'server_id': 'fakeuuid',
-                            'host_name': None,
-                            'id': '1',
-                            'volume_id': '1'
-                        }
-                    ],
+                    'multiattach': False,
+                    'attachments': [],
                     'user_id': 'fakeuser',
                     'volume_type': 'vol_type_name',
                     'snapshot_id': None,
@@ -699,6 +829,10 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         req = fakes.HTTPRequest.blank('/v2/volumes/detail')
         admin_ctx = context.RequestContext('admin', 'fakeproject', True)
@@ -707,7 +841,7 @@ class VolumeApiTest(test.TestCase):
         expected = {
             'volumes': [
                 {
-                    'status': 'fakestatus',
+                    'status': 'in-use',
                     'description': 'displaydesc',
                     'encrypted': False,
                     'availability_zone': 'fakeaz',
@@ -715,10 +849,12 @@ class VolumeApiTest(test.TestCase):
                     'consistencygroup_id': None,
                     'name': 'displayname',
                     'replication_status': 'disabled',
+                    'multiattach': False,
                     'attachments': [
                         {
+                            'attachment_id': attachment['id'],
                             'device': '/',
-                            'server_id': 'fakeuuid',
+                            'server_id': stubs.FAKE_UUID,
                             'host_name': None,
                             'id': '1',
                             'volume_id': '1'
@@ -1116,15 +1252,8 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'displayname',
                 'replication_status': 'disabled',
-                'attachments': [
-                    {
-                        'device': '/',
-                        'server_id': 'fakeuuid',
-                        'host_name': None,
-                        'id': '1',
-                        'volume_id': '1'
-                    }
-                ],
+                'multiattach': False,
+                'attachments': [],
                 'user_id': 'fakeuser',
                 'volume_type': 'vol_type_name',
                 'snapshot_id': None,
@@ -1167,6 +1296,7 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'displayname',
                 'replication_status': 'disabled',
+                'multiattach': False,
                 'attachments': [],
                 'user_id': 'fakeuser',
                 'volume_type': 'vol_type_name',
@@ -1211,6 +1341,10 @@ class VolumeApiTest(test.TestCase):
                                         {"readonly": "True",
                                          "invisible_key": "invisible_value"},
                                         False)
+        values = {'volume_id': '1', }
+        attachment = db.volume_attach(context.get_admin_context(), values)
+        db.volume_attached(context.get_admin_context(),
+                           attachment['id'], stubs.FAKE_UUID, None, '/')
 
         req = fakes.HTTPRequest.blank('/v2/volumes/1')
         admin_ctx = context.RequestContext('admin', 'fakeproject', True)
@@ -1218,7 +1352,7 @@ class VolumeApiTest(test.TestCase):
         res_dict = self.controller.show(req, '1')
         expected = {
             'volume': {
-                'status': 'fakestatus',
+                'status': 'in-use',
                 'description': 'displaydesc',
                 'encrypted': False,
                 'availability_zone': 'fakeaz',
@@ -1226,10 +1360,12 @@ class VolumeApiTest(test.TestCase):
                 'consistencygroup_id': None,
                 'name': 'displayname',
                 'replication_status': 'disabled',
+                'multiattach': False,
                 'attachments': [
                     {
+                        'attachment_id': attachment['id'],
                         'device': '/',
-                        'server_id': 'fakeuuid',
+                        'server_id': stubs.FAKE_UUID,
                         'host_name': None,
                         'id': '1',
                         'volume_id': '1'
diff --git a/cinder/tests/test_backup.py b/cinder/tests/test_backup.py
index 2ebbb10..26dbdb2 100644
--- a/cinder/tests/test_backup.py
+++ b/cinder/tests/test_backup.py
@@ -104,6 +104,13 @@ class BaseBackupTest(test.TestCase):
         vol['attach_status'] = 'detached'
         return db.volume_create(self.ctxt, vol)['id']
 
+    def _create_volume_attach(self, volume_id):
+        values = {'volume_id': volume_id,
+                  'attach_status': 'attached', }
+        attachment = db.volume_attach(self.ctxt, values)
+        db.volume_attached(self.ctxt, attachment['id'], None, 'testhost',
+                           '/dev/vd0')
+
     def _create_exported_record_entry(self, vol_size=1):
         """Create backup metadata export entry."""
         vol_id = self._create_volume_db_entry(status='available',
@@ -137,8 +144,12 @@ class BackupTestCase(BaseBackupTest):
         """Make sure stuck volumes and backups are reset to correct
         states when backup_manager.init_host() is called
         """
-        vol1_id = self._create_volume_db_entry(status='backing-up')
-        vol2_id = self._create_volume_db_entry(status='restoring-backup')
+        vol1_id = self._create_volume_db_entry()
+        self._create_volume_attach(vol1_id)
+        db.volume_update(self.ctxt, vol1_id, {'status': 'backing-up'})
+        vol2_id = self._create_volume_db_entry()
+        self._create_volume_attach(vol2_id)
+        db.volume_update(self.ctxt, vol2_id, {'status': 'restoring-backup'})
         backup1_id = self._create_backup_db_entry(status='creating')
         backup2_id = self._create_backup_db_entry(status='restoring')
         backup3_id = self._create_backup_db_entry(status='deleting')
diff --git a/cinder/tests/test_db_api.py b/cinder/tests/test_db_api.py
index 899a4bc..2a0f53c 100644
--- a/cinder/tests/test_db_api.py
+++ b/cinder/tests/test_db_api.py
@@ -274,26 +274,36 @@ class DBAPIVolumeTestCase(BaseTest):
     def test_volume_attached_to_instance(self):
         volume = db.volume_create(self.ctxt, {'host': 'host1'})
         instance_uuid = 'aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa'
-        db.volume_attached(self.ctxt, volume['id'],
+        values = {'volume_id': volume['id'],
+                  'instance_uuid': instance_uuid,
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.ctxt, values)
+        db.volume_attached(self.ctxt, attachment['id'],
                            instance_uuid, None, '/tmp')
         volume = db.volume_get(self.ctxt, volume['id'])
-        self.assertEqual(volume['status'], 'in-use')
-        self.assertEqual(volume['mountpoint'], '/tmp')
-        self.assertEqual(volume['attach_status'], 'attached')
-        self.assertEqual(volume['instance_uuid'], instance_uuid)
-        self.assertIsNone(volume['attached_host'])
+        attachment = db.volume_attachment_get(self.ctxt, attachment['id'])
+        self.assertEqual('in-use', volume['status'])
+        self.assertEqual('/tmp', attachment['mountpoint'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
 
     def test_volume_attached_to_host(self):
         volume = db.volume_create(self.ctxt, {'host': 'host1'})
         host_name = 'fake_host'
-        db.volume_attached(self.ctxt, volume['id'],
+        values = {'volume_id': volume['id'],
+                  'attached_host': host_name,
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.ctxt, values)
+        db.volume_attached(self.ctxt, attachment['id'],
                            None, host_name, '/tmp')
         volume = db.volume_get(self.ctxt, volume['id'])
-        self.assertEqual(volume['status'], 'in-use')
-        self.assertEqual(volume['mountpoint'], '/tmp')
-        self.assertEqual(volume['attach_status'], 'attached')
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertEqual(volume['attached_host'], host_name)
+        attachment = db.volume_attachment_get(self.ctxt, attachment['id'])
+        self.assertEqual('in-use', volume['status'])
+        self.assertEqual('/tmp', attachment['mountpoint'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertIsNone(attachment['instance_uuid'])
+        self.assertEqual(attachment['attached_host'], host_name)
 
     def test_volume_data_get_for_host(self):
         for i in xrange(3):
@@ -318,28 +328,38 @@ class DBAPIVolumeTestCase(BaseTest):
 
     def test_volume_detached_from_instance(self):
         volume = db.volume_create(self.ctxt, {})
-        db.volume_attached(self.ctxt, volume['id'],
-                           'aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa',
+        instance_uuid = 'aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa'
+        values = {'volume_id': volume['id'],
+                  'instance_uuid': instance_uuid,
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.ctxt, values)
+        db.volume_attached(self.ctxt, attachment['id'],
+                           instance_uuid,
                            None, '/tmp')
-        db.volume_detached(self.ctxt, volume['id'])
+        db.volume_detached(self.ctxt, volume['id'], attachment['id'])
         volume = db.volume_get(self.ctxt, volume['id'])
+        self.assertRaises(exception.VolumeAttachmentNotFound,
+                          db.volume_attachment_get,
+                          self.ctxt,
+                          attachment['id'])
         self.assertEqual('available', volume['status'])
-        self.assertEqual('detached', volume['attach_status'])
-        self.assertIsNone(volume['mountpoint'])
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertIsNone(volume['attached_host'])
 
     def test_volume_detached_from_host(self):
         volume = db.volume_create(self.ctxt, {})
-        db.volume_attached(self.ctxt, volume['id'],
-                           None, 'fake_host', '/tmp')
-        db.volume_detached(self.ctxt, volume['id'])
+        host_name = 'fake_host'
+        values = {'volume_id': volume['id'],
+                  'attach_host': host_name,
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.ctxt, values)
+        db.volume_attached(self.ctxt, attachment['id'],
+                           None, host_name, '/tmp')
+        db.volume_detached(self.ctxt, volume['id'], attachment['id'])
         volume = db.volume_get(self.ctxt, volume['id'])
+        self.assertRaises(exception.VolumeAttachmentNotFound,
+                          db.volume_attachment_get,
+                          self.ctxt,
+                          attachment['id'])
         self.assertEqual('available', volume['status'])
-        self.assertEqual('detached', volume['attach_status'])
-        self.assertIsNone(volume['mountpoint'])
-        self.assertIsNone(volume['instance_uuid'])
-        self.assertIsNone(volume['attached_host'])
 
     def test_volume_get(self):
         volume = db.volume_create(self.ctxt, {})
diff --git a/cinder/tests/test_hp3par.py b/cinder/tests/test_hp3par.py
index 8a0a70a..d60f18b 100644
--- a/cinder/tests/test_hp3par.py
+++ b/cinder/tests/test_hp3par.py
@@ -1058,20 +1058,25 @@ class HP3PARBaseDriver(object):
         # setup_mock_client drive with default configuration
         # and return the mock HTTP 3PAR client
         mock_client = self.setup_driver()
-        self.driver.detach_volume(context.get_admin_context(), self.volume)
-        expected = [
-            mock.call.removeVolumeMetaData(
-                self.VOLUME_3PAR_NAME,
-                'HPQ-CS-instance_uuid')]
-
-        mock_client.assert_has_calls(expected)
-
-        # test the exception
-        mock_client.removeVolumeMetaData.side_effect = Exception('Custom ex')
-        self.assertRaises(exception.CinderException,
-                          self.driver.detach_volume,
-                          context.get_admin_context(),
-                          self.volume)
+        with mock.patch.object(hpcommon.HP3PARCommon,
+                               '_create_client') as mock_create_client:
+            mock_create_client.return_value = mock_client
+            self.driver.detach_volume(context.get_admin_context(), self.volume,
+                                      None)
+            expected = [
+                mock.call.removeVolumeMetaData(
+                    self.VOLUME_3PAR_NAME,
+                    'HPQ-CS-instance_uuid')]
+
+            mock_client.assert_has_calls(expected)
+
+            # test the exception
+            mock_client.removeVolumeMetaData.side_effect = Exception(
+                'Custom ex')
+            self.assertRaises(exception.CinderException,
+                              self.driver.detach_volume,
+                              context.get_admin_context(),
+                              self.volume, None)
 
     def test_create_snapshot(self):
         # setup_mock_client drive with default configuration
diff --git a/cinder/tests/test_migrations.py b/cinder/tests/test_migrations.py
index 8438cd3..877cc80 100644
--- a/cinder/tests/test_migrations.py
+++ b/cinder/tests/test_migrations.py
@@ -1299,7 +1299,7 @@ class TestMigrations(test.TestCase):
 
             self.assertEqual(4, num_defaults)
 
-    def test_migration_032(self):
+    def test_migration_027(self):
         """Test adding volume_type_projects table works correctly."""
         for (key, engine) in self.engines.items():
             migration_api.version_control(engine,
@@ -1348,3 +1348,39 @@ class TestMigrations(test.TestCase):
                                             metadata,
                                             autoload=True)
             self.assertNotIn('is_public', volume_types.c)
+
+    def _check_28(self, engine, data):
+        volumes = db_utils.get_table(engine, 'volumes')
+        self.assertNotIn('instance_uuid', volumes.c)
+        self.assertNotIn('attached_host', volumes.c)
+        self.assertNotIn('attach_time', volumes.c)
+        self.assertNotIn('mountpoint', volumes.c)
+        self.assertIsInstance(volumes.c.multiattach.type,
+                              self.BOOL_TYPE)
+
+        attachments = db_utils.get_table(engine, 'volume_attachment')
+        self.assertIsInstance(attachments.c.attach_mode.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(attachments.c.instance_uuid.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(attachments.c.attached_host.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(attachments.c.mountpoint.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(attachments.c.attach_status.type,
+                              sqlalchemy.types.VARCHAR)
+
+    def _post_downgrade_028(self, engine):
+        self.assertFalse(engine.dialect.has_table(engine.connect(),
+                                                  "volume_attachment"))
+        volumes = db_utils.get_table(engine, 'volumes')
+        self.assertNotIn('multiattach', volumes.c)
+        self.assertIsInstance(volumes.c.instance_uuid.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(volumes.c.attached_host.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(volumes.c.attach_time.type,
+                              sqlalchemy.types.VARCHAR)
+        self.assertIsInstance(volumes.c.mountpoint.type,
+                              sqlalchemy.types.VARCHAR)
+
diff --git a/cinder/tests/test_volume.py b/cinder/tests/test_volume.py
index b8acbf3..d513f42 100644
--- a/cinder/tests/test_volume.py
+++ b/cinder/tests/test_volume.py
@@ -393,7 +393,6 @@ class VolumeTestCase(BaseVolumeTestCase):
             'volume_id': volume_id,
             'volume_type': None,
             'snapshot_id': None,
-            'instance_uuid': None,
             'user_id': 'fake',
             'launched_at': 'DONTCARE',
             'size': 1,
@@ -1403,14 +1402,15 @@ class VolumeTestCase(BaseVolumeTestCase):
                                            **self.volume_params)
         volume_id = volume['id']
         self.volume.create_volume(self.context, volume_id)
-        self.volume.attach_volume(self.context, volume_id, instance_uuid,
-                                  None, mountpoint, 'ro')
+        attachment = self.volume.attach_volume(self.context, volume_id,
+                                               instance_uuid, None,
+                                               mountpoint, 'ro')
         vol = db.volume_get(context.get_admin_context(), volume_id)
         self.assertEqual(vol['status'], "in-use")
-        self.assertEqual(vol['attach_status'], "attached")
-        self.assertEqual(vol['mountpoint'], mountpoint)
-        self.assertEqual(vol['instance_uuid'], instance_uuid)
-        self.assertIsNone(vol['attached_host'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
         admin_metadata = vol['volume_admin_metadata']
         self.assertEqual(len(admin_metadata), 2)
         expected = dict(readonly='True', attached_mode='ro')
@@ -1421,15 +1421,220 @@ class VolumeTestCase(BaseVolumeTestCase):
         connector = {'initiator': 'iqn.2012-07.org.fake:01'}
         conn_info = self.volume.initialize_connection(self.context,
                                                       volume_id, connector)
-        self.assertEqual(conn_info['data']['access_mode'], 'ro')
+        self.assertEqual('ro', conn_info['data']['access_mode'])
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('available', vol['status'])
+
+        self.volume.delete_volume(self.context, volume_id)
+        self.assertRaises(exception.VolumeNotFound,
+                          db.volume_get,
+                          self.context,
+                          volume_id)
+
+    def test_detach_invalid_attachment_id(self):
+        """Make sure if the attachment id isn't found we raise."""
+        attachment_id = "notfoundid"
+        volume_id = "abc123"
+        self.assertRaises(exception.VolumeAttachmentNotFound,
+                          self.volume.detach_volume,
+                          self.context,
+                          volume_id,
+                          attachment_id)
+
+    def test_run_attach_detach_volume_for_instance_no_attachment_id(self):
+        """Make sure volume can be attached and detached from instance."""
+        mountpoint = "/dev/sdf"
+        # attach volume to the instance then to detach
+        instance_uuid = '12345678-1234-5678-1234-567812345678'
+        instance_uuid_2 = '12345678-4321-8765-4321-567812345678'
+        volume = tests_utils.create_volume(self.context,
+                                           admin_metadata={'readonly': 'True'},
+                                           multiattach=True,
+                                           **self.volume_params)
+        volume_id = volume['id']
+        self.volume.create_volume(self.context, volume_id)
+        attachment = self.volume.attach_volume(self.context, volume_id,
+                                               instance_uuid, None,
+                                               mountpoint, 'ro')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
+        admin_metadata = vol['volume_admin_metadata']
+        self.assertEqual(2, len(admin_metadata))
+        expected = dict(readonly='True', attached_mode='ro')
+        ret = {}
+        for item in admin_metadata:
+            ret.update({item['key']: item['value']})
+        self.assertDictMatch(ret, expected)
+        attachment2 = self.volume.attach_volume(self.context, volume_id,
+                                                instance_uuid_2, None,
+                                                mountpoint, 'ro')
 
+        connector = {'initiator': 'iqn.2012-07.org.fake:01'}
+        conn_info = self.volume.initialize_connection(self.context,
+                                                      volume_id, connector)
+        self.assertEqual('ro', conn_info['data']['access_mode'])
         self.assertRaises(exception.VolumeAttached,
                           self.volume.delete_volume,
                           self.context,
                           volume_id)
+
+        self.assertRaises(exception.InvalidVolume,
+                          self.volume.detach_volume,
+                          self.context, volume_id)
+
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('in-use', vol['status'])
+
+        self.volume.detach_volume(self.context, volume_id, attachment2['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('available', vol['status'])
+
+        attachment = self.volume.attach_volume(self.context, volume_id,
+                                               instance_uuid, None,
+                                               mountpoint, 'ro')
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('in-use', vol['status'])
         self.volume.detach_volume(self.context, volume_id)
         vol = db.volume_get(self.context, volume_id)
-        self.assertEqual(vol['status'], "available")
+        self.assertEqual('available', vol['status'])
+
+        self.volume.delete_volume(self.context, volume_id)
+        self.assertRaises(exception.VolumeNotFound,
+                          db.volume_get,
+                          self.context,
+                          volume_id)
+
+    def test_run_attach_detach_multiattach_volume_for_instances(self):
+        """Make sure volume can be attached to multiple instances."""
+        mountpoint = "/dev/sdf"
+        # attach volume to the instance then to detach
+        instance_uuid = '12345678-1234-5678-1234-567812345678'
+        volume = tests_utils.create_volume(self.context,
+                                           admin_metadata={'readonly': 'True'},
+                                           multiattach=True,
+                                           **self.volume_params)
+        volume_id = volume['id']
+        self.volume.create_volume(self.context, volume_id)
+        attachment = self.volume.attach_volume(self.context, volume_id,
+                                               instance_uuid, None,
+                                               mountpoint, 'ro')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual(True, vol['multiattach'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
+        admin_metadata = vol['volume_admin_metadata']
+        self.assertEqual(2, len(admin_metadata))
+        expected = dict(readonly='True', attached_mode='ro')
+        ret = {}
+        for item in admin_metadata:
+            ret.update({item['key']: item['value']})
+        self.assertDictMatch(ret, expected)
+        connector = {'initiator': 'iqn.2012-07.org.fake:01'}
+        conn_info = self.volume.initialize_connection(self.context,
+                                                      volume_id, connector)
+        self.assertEqual('ro', conn_info['data']['access_mode'])
+
+        instance2_uuid = '12345678-1234-5678-1234-567812345000'
+        mountpoint2 = "/dev/sdx"
+        attachment2 = self.volume.attach_volume(self.context, volume_id,
+                                                instance2_uuid, None,
+                                                mountpoint2, 'ro')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual(True, vol['multiattach'])
+        self.assertEqual('attached', attachment2['attach_status'])
+        self.assertEqual(mountpoint2, attachment2['mountpoint'])
+        self.assertEqual(instance2_uuid, attachment2['instance_uuid'])
+        self.assertIsNone(attachment2['attached_host'])
+        self.assertNotEqual(attachment, attachment2)
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('in-use', vol['status'])
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+
+        self.volume.detach_volume(self.context, volume_id, attachment2['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('available', vol['status'])
+
+        self.volume.delete_volume(self.context, volume_id)
+        self.assertRaises(exception.VolumeNotFound,
+                          db.volume_get,
+                          self.context,
+                          volume_id)
+
+    def test_attach_detach_not_multiattach_volume_for_instances(self):
+        """Make sure volume can't be attached to more than one instance."""
+        mountpoint = "/dev/sdf"
+        # attach volume to the instance then to detach
+        instance_uuid = '12345678-1234-5678-1234-567812345678'
+        volume = tests_utils.create_volume(self.context,
+                                           admin_metadata={'readonly': 'True'},
+                                           multiattach=False,
+                                           **self.volume_params)
+        volume_id = volume['id']
+        self.volume.create_volume(self.context, volume_id)
+        attachment = self.volume.attach_volume(self.context, volume_id,
+                                               instance_uuid, None,
+                                               mountpoint, 'ro')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual(False, vol['multiattach'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
+        admin_metadata = vol['volume_admin_metadata']
+        self.assertEqual(2, len(admin_metadata))
+        expected = dict(readonly='True', attached_mode='ro')
+        ret = {}
+        for item in admin_metadata:
+            ret.update({item['key']: item['value']})
+        self.assertDictMatch(ret, expected)
+        connector = {'initiator': 'iqn.2012-07.org.fake:01'}
+        conn_info = self.volume.initialize_connection(self.context,
+                                                      volume_id, connector)
+        self.assertEqual('ro', conn_info['data']['access_mode'])
+
+        instance2_uuid = '12345678-1234-5678-1234-567812345000'
+        mountpoint2 = "/dev/sdx"
+        self.assertRaises(exception.InvalidVolume,
+                          self.volume.attach_volume,
+                          self.context,
+                          volume_id,
+                          instance2_uuid,
+                          None,
+                          mountpoint2, 'ro')
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('available', vol['status'])
 
         self.volume.delete_volume(self.context, volume_id)
         self.assertRaises(exception.VolumeNotFound,
@@ -1446,17 +1651,17 @@ class VolumeTestCase(BaseVolumeTestCase):
             **self.volume_params)
         volume_id = volume['id']
         self.volume.create_volume(self.context, volume_id)
-        self.volume.attach_volume(self.context, volume_id, None,
-                                  'fake_host', mountpoint, 'rw')
+        attachment = self.volume.attach_volume(self.context, volume_id, None,
+                                               'fake_host', mountpoint, 'rw')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['status'], "in-use")
-        self.assertEqual(vol['attach_status'], "attached")
-        self.assertEqual(vol['mountpoint'], mountpoint)
-        self.assertIsNone(vol['instance_uuid'])
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertIsNone(attachment['instance_uuid'])
         # sanitized, conforms to RFC-952 and RFC-1123 specs.
-        self.assertEqual(vol['attached_host'], 'fake-host')
+        self.assertEqual(attachment['attached_host'], 'fake-host')
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 2)
+        self.assertEqual(2, len(admin_metadata))
         expected = dict(readonly='False', attached_mode='rw')
         ret = {}
         for item in admin_metadata:
@@ -1466,13 +1671,75 @@ class VolumeTestCase(BaseVolumeTestCase):
         connector = {'initiator': 'iqn.2012-07.org.fake:01'}
         conn_info = self.volume.initialize_connection(self.context,
                                                       volume_id, connector)
+        self.assertEqual('rw', conn_info['data']['access_mode'])
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual(vol['status'], "available")
+
+        self.volume.delete_volume(self.context, volume_id)
+        self.assertRaises(exception.VolumeNotFound,
+                          db.volume_get,
+                          self.context,
+                          volume_id)
+
+    def test_run_attach_detach_multiattach_volume_for_hosts(self):
+        """Make sure volume can be attached and detached from hosts."""
+        mountpoint = "/dev/sdf"
+        volume = tests_utils.create_volume(
+            self.context,
+            admin_metadata={'readonly': 'False'},
+            multiattach=True,
+            **self.volume_params)
+        volume_id = volume['id']
+        self.volume.create_volume(self.context, volume_id)
+        attachment = self.volume.attach_volume(self.context, volume_id, None,
+                                               'fake_host', mountpoint, 'rw')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual(True, vol['multiattach'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertIsNone(attachment['instance_uuid'])
+        # sanitized, conforms to RFC-952 and RFC-1123 specs.
+        self.assertEqual(attachment['attached_host'], 'fake-host')
+        admin_metadata = vol['volume_admin_metadata']
+        self.assertEqual(2, len(admin_metadata))
+        expected = dict(readonly='False', attached_mode='rw')
+        ret = {}
+        for item in admin_metadata:
+            ret.update({item['key']: item['value']})
+        self.assertDictMatch(ret, expected)
+        connector = {'initiator': 'iqn.2012-07.org.fake:01'}
+        conn_info = self.volume.initialize_connection(self.context,
+                                                      volume_id, connector)
         self.assertEqual(conn_info['data']['access_mode'], 'rw')
 
+        mountpoint2 = "/dev/sdx"
+        attachment2 = self.volume.attach_volume(self.context, volume_id, None,
+                                                'fake_host2', mountpoint2,
+                                                'rw')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', attachment2['attach_status'])
+        self.assertEqual(mountpoint2, attachment2['mountpoint'])
+        self.assertIsNone(attachment2['instance_uuid'])
+        # sanitized, conforms to RFC-952 and RFC-1123 specs.
+        self.assertEqual('fake-host2', attachment2['attached_host'])
+
         self.assertRaises(exception.VolumeAttached,
                           self.volume.delete_volume,
                           self.context,
                           volume_id)
-        self.volume.detach_volume(self.context, volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual(vol['status'], "in-use")
+
+        self.volume.detach_volume(self.context, volume_id, attachment2['id'])
         vol = db.volume_get(self.context, volume_id)
         self.assertEqual(vol['status'], "available")
 
@@ -1482,6 +1749,69 @@ class VolumeTestCase(BaseVolumeTestCase):
                           self.context,
                           volume_id)
 
+    def test_run_attach_detach_not_multiattach_volume_for_hosts(self):
+        """Make sure volume can't be attached to more than one host."""
+        mountpoint = "/dev/sdf"
+        volume = tests_utils.create_volume(
+            self.context,
+            admin_metadata={'readonly': 'False'},
+            multiattach=False,
+            **self.volume_params)
+        volume_id = volume['id']
+        self.volume.create_volume(self.context, volume_id)
+        attachment = self.volume.attach_volume(self.context, volume_id, None,
+                                               'fake_host', mountpoint, 'rw')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual(False, vol['multiattach'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertIsNone(attachment['instance_uuid'])
+        # sanitized, conforms to RFC-952 and RFC-1123 specs.
+        self.assertEqual(attachment['attached_host'], 'fake-host')
+        admin_metadata = vol['volume_admin_metadata']
+        self.assertEqual(2, len(admin_metadata))
+        expected = dict(readonly='False', attached_mode='rw')
+        ret = {}
+        for item in admin_metadata:
+            ret.update({item['key']: item['value']})
+        self.assertDictMatch(ret, expected)
+        connector = {'initiator': 'iqn.2012-07.org.fake:01'}
+        conn_info = self.volume.initialize_connection(self.context,
+                                                      volume_id, connector)
+        self.assertEqual('rw', conn_info['data']['access_mode'])
+
+        mountpoint2 = "/dev/sdx"
+        self.assertRaises(exception.InvalidVolume,
+                          self.volume.attach_volume,
+                          self.context,
+                          volume_id,
+                          None,
+                          'fake_host2',
+                          mountpoint2,
+                          'rw')
+        vol = db.volume_get(context.get_admin_context(), volume_id)
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', attachment['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertIsNone(attachment['instance_uuid'])
+        # sanitized, conforms to RFC-952 and RFC-1123 specs.
+        self.assertEqual('fake-host', attachment['attached_host'])
+
+        self.assertRaises(exception.VolumeAttached,
+                          self.volume.delete_volume,
+                          self.context,
+                          volume_id)
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
+        vol = db.volume_get(self.context, volume_id)
+        self.assertEqual('available', vol['status'])
+
+        self.volume.delete_volume(self.context, volume_id)
+        self.assertRaises(exception.VolumeNotFound,
+                          db.volume_get,
+                          self.context,
+                          volume_id)
+
     def test_run_attach_detach_volume_with_attach_mode(self):
         instance_uuid = '12345678-1234-5678-1234-567812345678'
         mountpoint = "/dev/sdf"
@@ -1489,21 +1819,18 @@ class VolumeTestCase(BaseVolumeTestCase):
                                            admin_metadata={'readonly': 'True'},
                                            **self.volume_params)
         volume_id = volume['id']
-        db.volume_update(self.context, volume_id, {'status': 'available',
-                                                   'mountpoint': None,
-                                                   'instance_uuid': None,
-                                                   'attached_host': None,
-                                                   'attached_mode': None})
+        db.volume_update(self.context, volume_id, {'status': 'available', })
         self.volume.attach_volume(self.context, volume_id, instance_uuid,
                                   None, mountpoint, 'ro')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['status'], "in-use")
-        self.assertEqual(vol['attach_status'], "attached")
-        self.assertEqual(vol['mountpoint'], mountpoint)
-        self.assertEqual(vol['instance_uuid'], instance_uuid)
-        self.assertIsNone(vol['attached_host'])
+        attachment = vol['volume_attachment'][0]
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', vol['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertEqual(instance_uuid, attachment['instance_uuid'])
+        self.assertIsNone(attachment['attached_host'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 2)
+        self.assertEqual(2, len(admin_metadata))
         expected = dict(readonly='True', attached_mode='ro')
         ret = {}
         for item in admin_metadata:
@@ -1512,30 +1839,31 @@ class VolumeTestCase(BaseVolumeTestCase):
         connector = {'initiator': 'iqn.2012-07.org.fake:01'}
         conn_info = self.volume.initialize_connection(self.context,
                                                       volume_id, connector)
-        self.assertEqual(conn_info['data']['access_mode'], 'ro')
 
-        self.volume.detach_volume(self.context, volume_id)
+        self.assertEqual('ro', conn_info['data']['access_mode'])
+
+        self.volume.detach_volume(self.context, volume_id, attachment['id'])
         vol = db.volume_get(self.context, volume_id)
-        self.assertEqual(vol['status'], "available")
-        self.assertEqual(vol['attach_status'], "detached")
-        self.assertIsNone(vol['mountpoint'])
-        self.assertIsNone(vol['instance_uuid'])
-        self.assertIsNone(vol['attached_host'])
+        attachment = vol['volume_attachment']
+        self.assertEqual('available', vol['status'])
+        self.assertEqual('detached', vol['attach_status'])
+        self.assertEqual(attachment, [])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 1)
-        self.assertEqual(admin_metadata[0]['key'], 'readonly')
-        self.assertEqual(admin_metadata[0]['value'], 'True')
+        self.assertEqual(1, len(admin_metadata))
+        self.assertEqual('readonly', admin_metadata[0]['key'])
+        self.assertEqual('True', admin_metadata[0]['value'])
 
         self.volume.attach_volume(self.context, volume_id, None,
                                   'fake_host', mountpoint, 'ro')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['status'], "in-use")
-        self.assertEqual(vol['attach_status'], "attached")
-        self.assertEqual(vol['mountpoint'], mountpoint)
-        self.assertIsNone(vol['instance_uuid'])
-        self.assertEqual(vol['attached_host'], 'fake-host')
+        attachment = vol['volume_attachment'][0]
+        self.assertEqual('in-use', vol['status'])
+        self.assertEqual('attached', vol['attach_status'])
+        self.assertEqual(mountpoint, attachment['mountpoint'])
+        self.assertIsNone(attachment['instance_uuid'])
+        self.assertEqual('fake-host', attachment['attached_host'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 2)
+        self.assertEqual(2, len(admin_metadata))
         expected = dict(readonly='True', attached_mode='ro')
         ret = {}
         for item in admin_metadata:
@@ -1544,19 +1872,19 @@ class VolumeTestCase(BaseVolumeTestCase):
         connector = {'initiator': 'iqn.2012-07.org.fake:01'}
         conn_info = self.volume.initialize_connection(self.context,
                                                       volume_id, connector)
-        self.assertEqual(conn_info['data']['access_mode'], 'ro')
+        self.assertEqual('ro', conn_info['data']['access_mode'])
 
-        self.volume.detach_volume(self.context, volume_id)
+        self.volume.detach_volume(self.context, volume_id,
+                                  attachment['id'])
         vol = db.volume_get(self.context, volume_id)
-        self.assertEqual(vol['status'], "available")
-        self.assertEqual(vol['attach_status'], "detached")
-        self.assertIsNone(vol['mountpoint'])
-        self.assertIsNone(vol['instance_uuid'])
-        self.assertIsNone(vol['attached_host'])
+        attachment = vol['volume_attachment']
+        self.assertEqual('available', vol['status'])
+        self.assertEqual('detached', vol['attach_status'])
+        self.assertEqual(attachment, [])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 1)
-        self.assertEqual(admin_metadata[0]['key'], 'readonly')
-        self.assertEqual(admin_metadata[0]['value'], 'True')
+        self.assertEqual(1, len(admin_metadata))
+        self.assertEqual('readonly', admin_metadata[0]['key'])
+        self.assertEqual('True', admin_metadata[0]['value'])
 
         self.volume.delete_volume(self.context, volume_id)
         self.assertRaises(exception.VolumeNotFound,
@@ -1582,10 +1910,10 @@ class VolumeTestCase(BaseVolumeTestCase):
                           mountpoint,
                           'rw')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['status'], "error_attaching")
-        self.assertEqual(vol['attach_status'], "detached")
+        self.assertEqual('error_attaching', vol['status'])
+        self.assertEqual('detached', vol['attach_status'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 2)
+        self.assertEqual(2, len(admin_metadata))
         expected = dict(readonly='True', attached_mode='rw')
         ret = {}
         for item in admin_metadata:
@@ -1602,10 +1930,10 @@ class VolumeTestCase(BaseVolumeTestCase):
                           mountpoint,
                           'rw')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['status'], "error_attaching")
-        self.assertEqual(vol['attach_status'], "detached")
+        self.assertEqual('error_attaching', vol['status'])
+        self.assertEqual('detached', vol['attach_status'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 2)
+        self.assertEqual(2, len(admin_metadata))
         expected = dict(readonly='True', attached_mode='rw')
         ret = {}
         for item in admin_metadata:
@@ -1631,11 +1959,11 @@ class VolumeTestCase(BaseVolumeTestCase):
                           mountpoint,
                           'rw')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['attach_status'], "detached")
+        self.assertEqual('detached', vol['attach_status'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 1)
-        self.assertEqual(admin_metadata[0]['key'], 'readonly')
-        self.assertEqual(admin_metadata[0]['value'], 'True')
+        self.assertEqual(1, len(admin_metadata))
+        self.assertEqual('readonly', admin_metadata[0]['key'])
+        self.assertEqual('True', admin_metadata[0]['value'])
 
         db.volume_update(self.context, volume_id, {'status': 'available'})
         self.assertRaises(exception.InvalidVolumeAttachMode,
@@ -1647,11 +1975,11 @@ class VolumeTestCase(BaseVolumeTestCase):
                           mountpoint,
                           'rw')
         vol = db.volume_get(context.get_admin_context(), volume_id)
-        self.assertEqual(vol['attach_status'], "detached")
+        self.assertEqual('detached', vol['attach_status'])
         admin_metadata = vol['volume_admin_metadata']
-        self.assertEqual(len(admin_metadata), 1)
-        self.assertEqual(admin_metadata[0]['key'], 'readonly')
-        self.assertEqual(admin_metadata[0]['value'], 'True')
+        self.assertEqual(1, len(admin_metadata))
+        self.assertEqual('readonly', admin_metadata[0]['key'])
+        self.assertEqual('True', admin_metadata[0]['value'])
 
     @mock.patch.object(cinder.volume.api.API, 'update')
     @mock.patch.object(db, 'volume_get')
@@ -1675,7 +2003,7 @@ class VolumeTestCase(BaseVolumeTestCase):
     def test_reserve_volume_bad_status(self):
         fake_volume = {
             'id': FAKE_UUID,
-            'status': 'in-use'
+            'status': 'attaching'
         }
 
         with mock.patch.object(db, 'volume_get') as mock_volume_get:
@@ -1686,20 +2014,31 @@ class VolumeTestCase(BaseVolumeTestCase):
                               fake_volume)
             self.assertTrue(mock_volume_get.called)
 
-    def test_unreserve_volume_success(self):
+    @mock.patch.object(db, 'volume_get')
+    @mock.patch.object(db, 'volume_attachment_get_used_by_volume_id')
+    @mock.patch.object(cinder.volume.api.API, 'update')
+    def test_unreserve_volume_success(self, volume_get,
+                                      volume_attachment_get_used_by_volume_id,
+                                      volume_update):
         fake_volume = {
             'id': FAKE_UUID,
             'status': 'attaching'
         }
+        fake_attachments = [{'volume_id': FAKE_UUID,
+                             'instance_uuid': 'fake_instance_uuid'}]
+
+        volume_get.return_value = fake_volume
+        volume_attachment_get_used_by_volume_id.return_value = fake_attachments
+        volume_update.return_value = fake_volume
 
-        with mock.patch.object(cinder.volume.api.API,
-                               'update') as mock_volume_update:
-            mock_volume_update.return_value = fake_volume
-            self.assertIsNone(cinder.volume.api.API().unreserve_volume(
-                self.context,
-                fake_volume
-            ))
-            self.assertTrue(mock_volume_update.called)
+        self.assertIsNone(cinder.volume.api.API().unreserve_volume(
+            self.context,
+            fake_volume
+        ))
+
+        self.assertTrue(volume_get.called)
+        self.assertTrue(volume_attachment_get_used_by_volume_id.called)
+        self.assertTrue(volume_update.called)
 
     def test_concurrent_volumes_get_different_targets(self):
         """Ensure multiple concurrent volumes get different targets."""
@@ -1962,7 +2301,11 @@ class VolumeTestCase(BaseVolumeTestCase):
         # create volume and attach to the instance
         volume = tests_utils.create_volume(self.context, **self.volume_params)
         self.volume.create_volume(self.context, volume['id'])
-        db.volume_attached(self.context, volume['id'], instance_uuid,
+        values = {'volume_id': volume['id'],
+                  'instance_uuid': instance_uuid,
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.context, values)
+        db.volume_attached(self.context, attachment['id'], instance_uuid,
                            None, '/dev/sda1')
 
         volume_api = cinder.volume.api.API()
@@ -1981,7 +2324,11 @@ class VolumeTestCase(BaseVolumeTestCase):
         # create volume and attach to the host
         volume = tests_utils.create_volume(self.context, **self.volume_params)
         self.volume.create_volume(self.context, volume['id'])
-        db.volume_attached(self.context, volume['id'], None,
+        values = {'volume_id': volume['id'],
+                  'attached_host': 'fake_host',
+                  'attach_status': 'attaching', }
+        attachment = db.volume_attach(self.context, values)
+        db.volume_attached(self.context, attachment['id'], None,
                            'fake_host', '/dev/sda1')
 
         volume_api = cinder.volume.api.API()
@@ -2408,8 +2755,11 @@ class VolumeTestCase(BaseVolumeTestCase):
 
         instance_uuid = '12345678-1234-5678-1234-567812345678'
         volume = tests_utils.create_volume(self.context, **self.volume_params)
+        attachment = db.volume_attach(self.context,
+                                      {'volume_id': volume['id'],
+                                       'attached_host': 'fake-host'})
         volume = db.volume_attached(
-            self.context, volume['id'], instance_uuid, 'fake-host', 'vdb')
+            self.context, attachment['id'], instance_uuid, 'fake-host', 'vdb')
         volume_api = cinder.volume.api.API()
         volume_api.begin_detaching(self.context, volume)
         volume = db.volume_get(self.context, volume['id'])
@@ -2792,9 +3142,12 @@ class VolumeTestCase(BaseVolumeTestCase):
         old_volume = tests_utils.create_volume(self.context, size=0,
                                                host=CONF.host,
                                                status=initial_status,
-                                               migration_status='migrating',
-                                               instance_uuid=instance_uuid,
-                                               attached_host=attached_host)
+                                               migration_status='migrating')
+        if status == 'in-use':
+            vol = tests_utils.attach_volume(self.context, old_volume['id'],
+                                            instance_uuid, attached_host,
+                                            '/dev/vda')
+            self.assertEqual(vol['status'], 'in-use')
         target_status = 'target:%s' % old_volume['id']
         new_volume = tests_utils.create_volume(self.context, size=0,
                                                host=CONF.host,
@@ -2809,14 +3162,16 @@ class VolumeTestCase(BaseVolumeTestCase):
         self.stubs.Set(self.volume.driver, 'attach_volume',
                        lambda *args, **kwargs: None)
 
-        self.volume.migrate_volume_completion(self.context,
-                                              old_volume['id'],
-                                              new_volume['id'])
+        with mock.patch.object(self.volume.driver, 'detach_volume'):
+            self.volume.migrate_volume_completion(self.context, old_volume[
+                'id'], new_volume['id'])
 
-        volume = db.volume_get(elevated, old_volume['id'])
-        self.assertEqual(volume['status'], status)
-        self.assertEqual(volume['attached_host'], attached_host)
-        self.assertEqual(volume['instance_uuid'], instance_uuid)
+        if status == 'in-use':
+            attachment = db.volume_attachment_get_by_instance_uuid(
+                self.context, old_volume['id'], instance_uuid)
+            self.assertIsNotNone(attachment)
+            self.assertEqual(attachment['attached_host'], attached_host)
+            self.assertEqual(attachment['instance_uuid'], instance_uuid)
 
     def test_migrate_volume_completion_retype_available(self):
         self._test_migrate_volume_completion('available', retyping=True)
@@ -3316,8 +3671,6 @@ class CopyVolumeToImageTestCase(BaseVolumeTestCase):
     def test_copy_volume_to_image_status_use(self):
         self.image_meta['id'] = 'a440c04b-79fa-479c-bed1-0b816eaec379'
         # creating volume testdata
-        self.volume_attrs['instance_uuid'] = 'b21f957d-a72f-4b93-b5a5-' \
-                                             '45b1161abb02'
         db.volume_create(self.context, self.volume_attrs)
 
         # start test
@@ -3326,7 +3679,7 @@ class CopyVolumeToImageTestCase(BaseVolumeTestCase):
                                          self.image_meta)
 
         volume = db.volume_get(self.context, self.volume_id)
-        self.assertEqual(volume['status'], 'in-use')
+        self.assertEqual(volume['status'], 'available')
 
     def test_copy_volume_to_image_exception(self):
         self.image_meta['id'] = FAKE_UUID
diff --git a/cinder/tests/test_volume_rpcapi.py b/cinder/tests/test_volume_rpcapi.py
index 82fe639..e80d2ea 100644
--- a/cinder/tests/test_volume_rpcapi.py
+++ b/cinder/tests/test_volume_rpcapi.py
@@ -208,7 +208,9 @@ class VolumeRpcAPITestCase(test.TestCase):
     def test_detach_volume(self):
         self._test_volume_api('detach_volume',
                               rpc_method='call',
-                              volume=self.fake_volume)
+                              volume=self.fake_volume,
+                              attachment_id='fake_uuid',
+                              version="1.20")
 
     def test_copy_volume_to_image(self):
         self._test_volume_api('copy_volume_to_image',
diff --git a/cinder/tests/utils.py b/cinder/tests/utils.py
index 4b23b0f..6f8bab9 100644
--- a/cinder/tests/utils.py
+++ b/cinder/tests/utils.py
@@ -17,6 +17,8 @@
 from cinder import context
 from cinder import db
 
+from oslo.utils import timeutils
+
 
 def get_test_admin_context():
     return context.get_admin_context()
@@ -61,6 +63,21 @@ def create_volume(ctxt,
     return db.volume_create(ctxt, vol)
 
 
+def attach_volume(ctxt, volume_id, instance_uuid, attached_host,
+                  mountpoint, mode='rw'):
+
+    now = timeutils.utcnow()
+    values = {}
+    values['volume_id'] = volume_id
+    values['attached_host'] = attached_host
+    values['mountpoint'] = mountpoint
+    values['attach_time'] = now
+
+    attachment = db.volume_attach(ctxt, values)
+    return db.volume_attached(ctxt, attachment['id'], instance_uuid,
+                              attached_host, mountpoint, mode)
+
+
 def create_snapshot(ctxt,
                     volume_id,
                     display_name='test_snapshot',
diff --git a/cinder/volume/api.py b/cinder/volume/api.py
index ba9a909..4c609a1 100644
--- a/cinder/volume/api.py
+++ b/cinder/volume/api.py
@@ -154,7 +154,8 @@ class API(base.Base):
                image_id=None, volume_type=None, metadata=None,
                availability_zone=None, source_volume=None,
                scheduler_hints=None, backup_source_volume=None,
-               source_replica=None, consistencygroup=None):
+               source_replica=None, consistencygroup=None,
+               cgsnapshot=None, multiattach=False):
 
         # NOTE(jdg): we can have a create without size if we're
         # doing a create from snap or volume.  Currently
@@ -227,7 +228,9 @@ class API(base.Base):
             'backup_source_volume': backup_source_volume,
             'source_replica': source_replica,
             'optional_args': {'is_quota_committed': False},
-            'consistencygroup': consistencygroup
+            'consistencygroup': consistencygroup,
+            'cgsnapshot': cgsnapshot,
+            'multiattach': multiattach,
         }
         try:
             flow_engine = create_volume.get_flow(self.scheduler_rpcapi,
@@ -443,6 +446,13 @@ class API(base.Base):
         volume = self.db.volume_get(context, volume['id'])
         if volume['status'] == 'available':
             self.update(context, volume, {"status": "attaching"})
+        elif volume['status'] == 'in-use':
+            if volume['multiattach']:
+                self.update(context, volume, {"status": "attaching"})
+            else:
+                msg = _("Volume must be multiattachable to reserve again.")
+                LOG.error(msg)
+                raise exception.InvalidVolume(reason=msg)
         else:
             msg = _("Volume status must be available to reserve")
             LOG.error(msg)
@@ -450,8 +460,14 @@ class API(base.Base):
 
     @wrap_check_policy
     def unreserve_volume(self, context, volume):
-        if volume['status'] == "attaching":
-            self.update(context, volume, {"status": "available"})
+        volume = self.db.volume_get(context, volume['id'])
+        if volume['status'] == 'attaching':
+            attaches = self.db.volume_attachment_get_used_by_volume_id(
+                context, volume['id'])
+            if attaches:
+                self.update(context, volume, {"status": "in-use"})
+            else:
+                self.update(context, volume, {"status": "available"})
 
     @wrap_check_policy
     def begin_detaching(self, context, volume):
@@ -501,8 +517,9 @@ class API(base.Base):
                                                 mode)
 
     @wrap_check_policy
-    def detach(self, context, volume):
-        return self.volume_rpcapi.detach_volume(context, volume)
+    def detach(self, context, volume, attachment_id):
+        return self.volume_rpcapi.detach_volume(context, volume,
+                                                attachment_id)
 
     @wrap_check_policy
     def initialize_connection(self, context, volume, connector):
diff --git a/cinder/volume/driver.py b/cinder/volume/driver.py
index 621307d..c528670 100644
--- a/cinder/volume/driver.py
+++ b/cinder/volume/driver.py
@@ -536,7 +536,7 @@ class BaseVD(object):
         """Callback for volume attached to instance or host."""
         pass
 
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
         """Callback for volume detached."""
         pass
 
diff --git a/cinder/volume/drivers/datera.py b/cinder/volume/drivers/datera.py
index 04dd3f7..53197de 100644
--- a/cinder/volume/drivers/datera.py
+++ b/cinder/volume/drivers/datera.py
@@ -114,7 +114,7 @@ class DateraDriver(san.SanISCSIDriver):
     def create_export(self, context, volume):
         return self._do_export(context, volume)
 
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
         try:
             self._issue_api_request('volumes', 'delete', resource=volume['id'],
                                     action='export')
diff --git a/cinder/volume/drivers/san/hp/hp_3par_common.py b/cinder/volume/drivers/san/hp/hp_3par_common.py
index 7681635..2c1ab74 100644
--- a/cinder/volume/drivers/san/hp/hp_3par_common.py
+++ b/cinder/volume/drivers/san/hp/hp_3par_common.py
@@ -151,10 +151,26 @@ class HP3PARCommon(object):
         2.0.21 - Remove bogus invalid snapCPG=None exception
         2.0.22 - HP 3PAR drivers should not claim to have 'infinite' space
         2.0.23 - Increase the hostname size from 23 to 31  Bug #1371242
+        2.0.24 - Add pools (hp3par_cpg now accepts a list of CPGs)
+        2.0.25 - Migrate without losing type settings bug #1356608
+        2.0.26 - Don't ignore extra-specs snap_cpg when missing cpg #1368972
+        2.0.27 - Fixing manage source-id error bug #1357075
+        2.0.28 - Removing locks bug #1381190
+        2.0.29 - Report a limitless cpg's stats better bug #1398651
+        2.0.30 - Update the minimum hp3parclient version bug #1402115
+        2.0.31 - Removed usage of host name cache #1398914
+        2.0.32 - Update LOG usage to fix translations.  bug #1384312
+        2.0.33 - Fix host persona to match WSAPI mapping bug #1403997
+        2.0.34 - Fix log messages to match guidelines. bug #1411370
+        2.0.35 - Fix default snapCPG for manage_existing bug #1393609
+        2.0.36 - Added support for dedup provisioning
+        2.0.37 - Added support for enabling Flash Cache
+        2.0.38 - Add stats for hp3par goodness_function and filter_function
+        2.0.39 - Added support for updated detach_volume attachment.
 
     """
 
-    VERSION = "2.0.23"
+    VERSION = "2.0.39"
 
     stats = {}
 
@@ -1334,7 +1350,12 @@ class HP3PARCommon(object):
             raise exception.VolumeBackendAPIException(data=msg)
 
     def attach_volume(self, volume, instance_uuid):
-        LOG.debug("Attach Volume\n%s" % pprint.pformat(volume))
+        """Save the instance UUID in the volume.
+
+           TODO: add support for multi-attach
+
+        """
+        LOG.debug("Attach Volume\n%s", pprint.pformat(volume))
         try:
             self.update_volume_key_value_pair(volume,
                                               'HPQ-CS-instance_uuid',
@@ -1343,8 +1364,13 @@ class HP3PARCommon(object):
             with excutils.save_and_reraise_exception():
                 LOG.error(_("Error attaching volume %s") % volume)
 
-    def detach_volume(self, volume):
-        LOG.debug("Detach Volume\n%s" % pprint.pformat(volume))
+    def detach_volume(self, volume, attachment=None):
+        """Remove the instance uuid from the volume.
+
+           TODO: add support for multi-attach.
+
+        """
+        LOG.debug("Detach Volume\n%s", pprint.pformat(volume))
         try:
             self.clear_volume_key_value_pair(volume, 'HPQ-CS-instance_uuid')
         except Exception:
diff --git a/cinder/volume/drivers/san/hp/hp_3par_fc.py b/cinder/volume/drivers/san/hp/hp_3par_fc.py
index abfa776..d7ebf0c 100644
--- a/cinder/volume/drivers/san/hp/hp_3par_fc.py
+++ b/cinder/volume/drivers/san/hp/hp_3par_fc.py
@@ -69,10 +69,17 @@ class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
         2.0.7 - Only one FC port is used when a single FC path
                 is present.  bug #1360001
         2.0.8 - Fixing missing login/logout around attach/detach bug #1367429
+        2.0.9 - Add support for pools with model update
+        2.0.10 - Migrate without losing type settings bug #1356608
+        2.0.11 - Removing locks bug #1381190
+        2.0.12 - Fix queryHost call to specify wwns bug #1398206
+        2.0.13 - Fix missing host name during attach bug #1398206
+        2.0.14 - Removed usage of host name cache #1398914
+        2.0.15 - Added support for updated detach_volume attachment.
 
     """
 
-    VERSION = "2.0.8"
+    VERSION = "2.0.15"
 
     def __init__(self, *args, **kwargs):
         super(HP3PARFCDriver, self).__init__(*args, **kwargs)
@@ -443,10 +450,10 @@ class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
             self.common.client_logout()
 
     @utils.synchronized('3par', external=True)
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
         self.common.client_login()
         try:
-            self.common.detach_volume(volume)
+            self.common.detach_volume(volume, attachment)
         finally:
             self.common.client_logout()
 
diff --git a/cinder/volume/drivers/san/hp/hp_3par_iscsi.py b/cinder/volume/drivers/san/hp/hp_3par_iscsi.py
index 3027df0..6e02cb4 100644
--- a/cinder/volume/drivers/san/hp/hp_3par_iscsi.py
+++ b/cinder/volume/drivers/san/hp/hp_3par_iscsi.py
@@ -74,10 +74,20 @@ class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
         2.0.5 - Added CHAP support, requires 3.1.3 MU1 firmware
                 and hp3parclient 3.1.0.
         2.0.6 - Fixing missing login/logout around attach/detach bug #1367429
+        2.0.7 - Add support for pools with model update
+        2.0.8 - Migrate without losing type settings bug #1356608
+        2.0.9 - Removing locks bug #1381190
+        2.0.10 - Add call to queryHost instead SSH based findHost #1398206
+        2.0.11 - Added missing host name during attach fix #1398206
+        2.0.12 - Removed usage of host name cache #1398914
+        2.0.13 - Update LOG usage to fix translations.  bug #1384312
+        2.0.14 - Do not allow a different iSCSI IP (hp3par_iscsi_ips) to be
+                 used during live-migration.  bug #1423958
+        2.0.15 - Added support for updated detach_volume attachment.
 
     """
 
-    VERSION = "2.0.6"
+    VERSION = "2.0.15"
 
     def __init__(self, *args, **kwargs):
         super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
@@ -651,10 +661,10 @@ class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
             self.common.client_logout()
 
     @utils.synchronized('3par', external=True)
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
         self.common.client_login()
         try:
-            self.common.detach_volume(volume)
+            self.common.detach_volume(volume, attachment)
         finally:
             self.common.client_logout()
 
diff --git a/cinder/volume/drivers/scality.py b/cinder/volume/drivers/scality.py
index 6b9182f..50fccfe 100644
--- a/cinder/volume/drivers/scality.py
+++ b/cinder/volume/drivers/scality.py
@@ -214,7 +214,7 @@ class ScalityDriver(driver.VolumeDriver):
         """Disallow connection from connector."""
         pass
 
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
         """Callback for volume detached."""
         pass
 
diff --git a/cinder/volume/drivers/solidfire.py b/cinder/volume/drivers/solidfire.py
index 15b2fd1..6172adc 100644
--- a/cinder/volume/drivers/solidfire.py
+++ b/cinder/volume/drivers/solidfire.py
@@ -739,7 +739,7 @@ class SolidFireDriver(SanISCSIDriver):
         if 'result' not in data:
             raise exception.SolidFireAPIDataException(data=data)
 
-    def detach_volume(self, context, volume):
+    def detach_volume(self, context, volume, attachment=None):
 
         LOG.debug("Entering SolidFire attach_volume...")
         sfaccount = self._get_sfaccount(volume['project_id'])
diff --git a/cinder/volume/flows/api/create_volume.py b/cinder/volume/flows/api/create_volume.py
index cd94321..ce36bdf 100644
--- a/cinder/volume/flows/api/create_volume.py
+++ b/cinder/volume/flows/api/create_volume.py
@@ -467,7 +467,8 @@ class EntryCreateTask(flow_utils.CinderTask):
         requires = ['availability_zone', 'description', 'metadata',
                     'name', 'reservations', 'size', 'snapshot_id',
                     'source_volid', 'volume_type_id', 'encryption_key_id',
-                    'source_replicaid', 'consistencygroup_id', ]
+                    'source_replicaid', 'consistencygroup_id',
+                    'multiattach']
         super(EntryCreateTask, self).__init__(addons=[ACTION],
                                               requires=requires)
         self.db = db
@@ -494,6 +495,7 @@ class EntryCreateTask(flow_utils.CinderTask):
             'display_description': kwargs.pop('description'),
             'display_name': kwargs.pop('name'),
             'replication_status': 'disabled',
+            'multiattach': kwargs.pop('multiattach'),
         }
 
         # Merge in the other required arguments which should provide the rest
diff --git a/cinder/volume/manager.py b/cinder/volume/manager.py
index 8859807..083c358 100644
--- a/cinder/volume/manager.py
+++ b/cinder/volume/manager.py
@@ -47,6 +47,7 @@ from cinder import context
 from cinder import exception
 from cinder import flow_utils
 from cinder.i18n import _
+from cinder.openstack.common.gettextutils import _LE, _LI, _LW
 from cinder.image import glance
 from cinder import manager
 from cinder.openstack.common import excutils
@@ -128,6 +129,25 @@ def locked_volume_operation(f):
     return lvo_inner1
 
 
+def locked_detach_operation(f):
+    """Lock decorator for volume detach operations.
+
+    Takes a named lock prior to executing the detach call.  The lock is named
+    with the operation executed and the id of the volume. This lock can then
+    be used by other operations to avoid operation conflicts on shared volumes.
+
+    This locking mechanism is only for detach calls.   We can't use the
+    locked_volume_operation, because detach requires an additional
+    attachment_id in the parameter list.
+    """
+    def ldo_inner1(inst, context, volume_id, attachment_id=None, **kwargs):
+        @utils.synchronized("%s-%s" % (volume_id, f.__name__), external=True)
+        def ldo_inner2(*_args, **_kwargs):
+            return f(*_args, **_kwargs)
+        return ldo_inner2(inst, context, volume_id, attachment_id, **kwargs)
+    return ldo_inner1
+
+
 def locked_snapshot_operation(f):
     """Lock decorator for snapshot operations.
 
@@ -153,7 +173,7 @@ def locked_snapshot_operation(f):
 class VolumeManager(manager.SchedulerDependentManager):
     """Manages attachable block storage devices."""
 
-    RPC_API_VERSION = '1.18'
+    RPC_API_VERSION = '1.23'
 
     target = messaging.Target(version=RPC_API_VERSION)
 
@@ -637,46 +657,47 @@ class VolumeManager(manager.SchedulerDependentManager):
             volume_metadata = self.db.volume_admin_metadata_get(
                 context.elevated(), volume_id)
             if volume['status'] == 'attaching':
-                if (volume['instance_uuid'] and volume['instance_uuid'] !=
-                        instance_uuid):
-                    msg = _("being attached by another instance")
-                    raise exception.InvalidVolume(reason=msg)
-                if (volume['attached_host'] and volume['attached_host'] !=
-                        host_name):
-                    msg = _("being attached by another host")
-                    raise exception.InvalidVolume(reason=msg)
                 if (volume_metadata.get('attached_mode') and
-                        volume_metadata.get('attached_mode') != mode):
+                   volume_metadata.get('attached_mode') != mode):
                     msg = _("being attached by different mode")
                     raise exception.InvalidVolume(reason=msg)
-            elif (not volume['migration_status'] and
-                  volume['status'] != "available"):
-                msg = _("status must be available or attaching")
+
+            if (volume['status'] == 'in-use' and not volume['multiattach']
+               and not volume['migration_status']):
+                msg = _("volume is already attached")
                 raise exception.InvalidVolume(reason=msg)
 
-            # TODO(jdg): attach_time column is currently varchar
-            # we should update this to a date-time object
-            # also consider adding detach_time?
+            attachment = None
+            host_name_sanitized = utils.sanitize_hostname(
+                host_name) if host_name else None
+            if instance_uuid:
+                attachment = \
+                    self.db.volume_attachment_get_by_instance_uuid(
+                        context, volume_id, instance_uuid)
+            else:
+                attachment = \
+                    self.db.volume_attachment_get_by_host(context, volume_id,
+                                                          host_name_sanitized)
+            if attachment is not None:
+                return
+
             self._notify_about_volume_usage(context, volume,
                                             "attach.start")
-            self.db.volume_update(context, volume_id,
-                                  {"instance_uuid": instance_uuid,
-                                   "attached_host": host_name,
-                                   "status": "attaching",
-                                   "attach_time": timeutils.strtime()})
-            self.db.volume_admin_metadata_update(context.elevated(),
-                                                 volume_id,
-                                                 {"attached_mode": mode},
-                                                 False)
+            values = {'volume_id': volume_id,
+                      'attach_status': 'attaching', }
 
+            attachment = self.db.volume_attach(context.elevated(), values)
+            volume_metadata = self.db.volume_admin_metadata_update(
+                context.elevated(), volume_id,
+                {"attached_mode": mode}, False)
+
+            attachment_id = attachment['id']
             if instance_uuid and not uuidutils.is_uuid_like(instance_uuid):
-                self.db.volume_update(context, volume_id,
-                                      {'status': 'error_attaching'})
+                self.db.volume_attachment_update(context, attachment_id,
+                                                 {'attach_status':
+                                                  'error_attaching'})
                 raise exception.InvalidUUID(uuid=instance_uuid)
 
-            host_name_sanitized = utils.sanitize_hostname(
-                host_name) if host_name else None
-
             volume = self.db.volume_get(context, volume_id)
 
             if volume_metadata.get('readonly') == 'True' and mode != 'ro':
@@ -684,6 +705,7 @@ class VolumeManager(manager.SchedulerDependentManager):
                                       {'status': 'error_attaching'})
                 raise exception.InvalidVolumeAttachMode(mode=mode,
                                                         volume_id=volume_id)
+
             try:
                 # NOTE(flaper87): Verify the driver is enabled
                 # before going forward. The exception will be caught
@@ -697,24 +719,55 @@ class VolumeManager(manager.SchedulerDependentManager):
                                           mountpoint)
             except Exception:
                 with excutils.save_and_reraise_exception():
-                    self.db.volume_update(context, volume_id,
-                                          {'status': 'error_attaching'})
+                    self.db.volume_attachment_update(
+                        context, attachment_id,
+                        {'attach_status': 'error_attaching'})
 
             volume = self.db.volume_attached(context.elevated(),
-                                             volume_id,
+                                             attachment_id,
                                              instance_uuid,
                                              host_name_sanitized,
-                                             mountpoint)
+                                             mountpoint,
+                                             mode)
             if volume['migration_status']:
                 self.db.volume_update(context, volume_id,
                                       {'migration_status': None})
             self._notify_about_volume_usage(context, volume, "attach.end")
+            return self.db.volume_attachment_get(context, attachment_id)
         return do_attach()
 
-    @locked_volume_operation
-    def detach_volume(self, context, volume_id):
+    @locked_detach_operation
+    def detach_volume(self, context, volume_id, attachment_id=None):
         """Updates db to show volume is detached."""
         # TODO(vish): refactor this into a more general "unreserve"
+        attachment = None
+        if attachment_id:
+            try:
+                attachment = self.db.volume_attachment_get(context,
+                                                           attachment_id)
+            except exception.VolumeAttachmentNotFound:
+                LOG.error(_LE("We couldn't find the volume attachment"
+                              " for volume %(volume_id)s and"
+                              " attachment id %(id)s"),
+                          {"volume_id": volume_id,
+                           "id": attachment_id})
+                raise
+        else:
+            # We can try and degrade gracefuly here by trying to detach
+            # a volume without the attachment_id here if the volume only has
+            # one attachment.  This is for backwards compatibility.
+            attachments = self.db.volume_attachment_get_used_by_volume_id(
+                context, volume_id)
+            if len(attachments) > 1:
+                # There are more than 1 attachments for this volume
+                # we have to have an attachment id.
+                msg = _("Volume %(id)s is attached to more than one instance"
+                        ".  A valid attachment_id must be passed to detach"
+                        " this volume") % {'id': volume_id}
+                LOG.error(msg)
+                raise exception.InvalidVolume(reason=msg)
+            else:
+                attachment = attachments[0]
 
         volume = self.db.volume_get(context, volume_id)
         self._notify_about_volume_usage(context, volume, "detach.start")
@@ -724,14 +777,15 @@ class VolumeManager(manager.SchedulerDependentManager):
             # and the volume status updated.
             utils.require_driver_initialized(self.driver)
 
-            self.driver.detach_volume(context, volume)
+            self.driver.detach_volume(context, volume, attachment)
         except Exception:
             with excutils.save_and_reraise_exception():
-                self.db.volume_update(context,
-                                      volume_id,
-                                      {'status': 'error_detaching'})
+                self.db.volume_attachment_update(
+                    context, attachment.get('id'),
+                    {'attach_status': 'error_detaching'})
 
-        self.db.volume_detached(context.elevated(), volume_id)
+        self.db.volume_detached(context.elevated(), volume_id,
+                                attachment.get('id'))
         self.db.volume_admin_metadata_delete(context.elevated(), volume_id,
                                              'attached_mode')
 
@@ -795,8 +849,7 @@ class VolumeManager(manager.SchedulerDependentManager):
             with excutils.save_and_reraise_exception():
                 payload['message'] = unicode(error)
         finally:
-            if (volume['instance_uuid'] is None and
-                    volume['attached_host'] is None):
+            if not volume['volume_attachment']:
                 self.db.volume_update(context, volume_id,
                                       {'status': 'available'})
             else:
@@ -1027,8 +1080,8 @@ class VolumeManager(manager.SchedulerDependentManager):
 
         # Copy the source volume to the destination volume
         try:
-            if (volume['instance_uuid'] is None and
-                    volume['attached_host'] is None):
+            attachments = volume['volume_attachment']
+            if not attachments:
                 self.driver.copy_volume_data(ctxt, volume, new_volume,
                                              remote='dest')
                 # The above call is synchronous so we complete the migration
@@ -1038,8 +1091,11 @@ class VolumeManager(manager.SchedulerDependentManager):
                 nova_api = compute.API()
                 # This is an async call to Nova, which will call the completion
                 # when it's done
-                nova_api.update_server_volume(ctxt, volume['instance_uuid'],
-                                              volume['id'], new_volume['id'])
+                for attachment in attachments:
+                    instance_uuid = attachment['instance_uuid']
+                    nova_api.update_server_volume(ctxt, instance_uuid,
+                                                  volume['id'],
+                                                  new_volume['id'])
         except Exception:
             with excutils.save_and_reraise_exception():
                 msg = _("Failed to copy volume %(vol1)s to %(vol2)s")
@@ -1053,8 +1109,8 @@ class VolumeManager(manager.SchedulerDependentManager):
                 new_volume['migration_status'] = None
 
     def _get_original_status(self, volume):
-        if (volume['instance_uuid'] is None and
-                volume['attached_host'] is None):
+        attachments = volume['volume_attachment']
+        if not attachments:
             return 'available'
         else:
             return 'in-use'
@@ -1117,12 +1173,13 @@ class VolumeManager(manager.SchedulerDependentManager):
         self.db.volume_update(ctxt, volume_id, updates)
 
         if 'in-use' in (status_update.get('status'), volume['status']):
-            rpcapi.attach_volume(ctxt,
-                                 volume,
-                                 volume['instance_uuid'],
-                                 volume['attached_host'],
-                                 volume['mountpoint'],
-                                 'rw')
+            attachments = volume['volume_attachment']
+            for attachment in attachments:
+                rpcapi.attach_volume(ctxt, volume,
+                                     attachment['instance_uuid'],
+                                     attachment['attached_host'],
+                                     attachment['mountpoint'],
+                                     'rw')
         return volume['id']
 
     def migrate_volume(self, ctxt, volume_id, host, force_host_copy=False,
diff --git a/cinder/volume/rpcapi.py b/cinder/volume/rpcapi.py
index 6c7638c..59dfbe4 100644
--- a/cinder/volume/rpcapi.py
+++ b/cinder/volume/rpcapi.py
@@ -57,6 +57,12 @@ class VolumeAPI(object):
         1.18 - Adds create_consistencygroup, delete_consistencygroup,
                create_cgsnapshot, and delete_cgsnapshot. Also adds
                the consistencygroup_id parameter in create_volume.
+        1.19 - Adds update_migrated_volume
+        1.20 - Adds support for sending objects over RPC in create_snapshot()
+               and delete_snapshot()
+        1.21 - Adds update_consistencygroup.
+        1.22 - Adds create_consistencygroup_from_src.
+        1.23 - Adds attachment_id to detach_volume
     '''
 
     BASE_RPC_API_VERSION = '1.0'
@@ -65,7 +71,7 @@ class VolumeAPI(object):
         super(VolumeAPI, self).__init__()
         target = messaging.Target(topic=CONF.volume_topic,
                                   version=self.BASE_RPC_API_VERSION)
-        self.client = rpc.get_client(target, '1.18')
+        self.client = rpc.get_client(target, '1.23')
 
     def create_consistencygroup(self, ctxt, group, host):
         new_host = utils.extract_host(host)
@@ -145,10 +151,11 @@ class VolumeAPI(object):
                           mountpoint=mountpoint,
                           mode=mode)
 
-    def detach_volume(self, ctxt, volume):
+    def detach_volume(self, ctxt, volume, attachment_id):
         new_host = utils.extract_host(volume['host'])
-        cctxt = self.client.prepare(server=new_host)
-        return cctxt.call(ctxt, 'detach_volume', volume_id=volume['id'])
+        cctxt = self.client.prepare(server=new_host, version='1.20')
+        return cctxt.call(ctxt, 'detach_volume', volume_id=volume['id'],
+                          attachment_id=attachment_id)
 
     def copy_volume_to_image(self, ctxt, volume, image_meta):
         new_host = utils.extract_host(volume['host'])
diff --git a/cinder/volume/utils.py b/cinder/volume/utils.py
index 8853904..d29de40 100644
--- a/cinder/volume/utils.py
+++ b/cinder/volume/utils.py
@@ -45,7 +45,6 @@ def _usage_from_volume(context, volume_ref, **kw):
     usage_info = dict(tenant_id=volume_ref['project_id'],
                       host=volume_ref['host'],
                       user_id=volume_ref['user_id'],
-                      instance_uuid=volume_ref['instance_uuid'],
                       availability_zone=volume_ref['availability_zone'],
                       volume_id=volume_ref['id'],
                       volume_type=volume_ref['volume_type_id'],
-- 
2.6.4

